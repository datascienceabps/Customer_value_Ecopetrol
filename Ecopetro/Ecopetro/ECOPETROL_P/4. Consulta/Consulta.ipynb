{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"XkQ3GLFC7Ci9"},"outputs":[{"name":"stdout","output_type":"stream","text":["boto3\n","No instalado boto3\n","es_lemmatizer\n","No instalado es_lemmatizer\n","pyodbc\n","No instalado pyodbc\n","prince\n","No instalado prince\n","pyLDAvis==2.1.2\n","No instalado pyLDAvis==2.1.2\n","gensim\n","ya instalado gensim\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/c0/8af2139d5658eccde11f45fd9d27046edb286fd60f5371e27870612287bd/boto3-1.17.111-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 6.3MB/s \n","\u001b[?25hCollecting es_lemmatizer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/5d/092e431b146bf7af6b816770403f776aebe32e944c44f5efe87c0e23094f/es_lemmatizer-0.2.1-py3-none-any.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 8.5MB/s \n","\u001b[?25hCollecting pyodbc\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/20/fce702980820fedf856c95787c20a5bbe2bbe4e141d56c7a5004fe6b06fa/pyodbc-4.0.31.tar.gz (280kB)\n","\u001b[K     |████████████████████████████████| 286kB 29.4MB/s \n","\u001b[?25hCollecting prince\n","  Downloading https://files.pythonhosted.org/packages/94/6c/491a3fabfd1ce75e285a4fe4200fccde5d83664733541a3a74c0b02e77fb/prince-0.7.1-py3-none-any.whl\n","Collecting pyLDAvis==2.1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 11.5MB/s \n","\u001b[?25hCollecting botocore\u003c1.21.0,\u003e=1.20.111\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/56/64570ac92c7cb88ad731dea4da4a83d3edc9f00a13a969ad826354ba5a58/botocore-1.20.111.tar.gz (7.9MB)\n","\u001b[K     |████████████████████████████████| 7.9MB 22.4MB/s \n","\u001b[?25hCollecting jmespath\u003c1.0.0,\u003e=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer\u003c0.5.0,\u003e=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from prince-\u003e-r paquetes.txt (line 4)) (3.2.2)\n","Requirement already satisfied: pandas\u003e=1.0.3 in /usr/local/lib/python3.7/dist-packages (from prince-\u003e-r paquetes.txt (line 4)) (1.1.5)\n","Requirement already satisfied: numpy\u003e=1.17.1 in /usr/local/lib/python3.7/dist-packages (from prince-\u003e-r paquetes.txt (line 4)) (1.19.5)\n","Requirement already satisfied: scipy\u003e=1.3.0 in /usr/local/lib/python3.7/dist-packages (from prince-\u003e-r paquetes.txt (line 4)) (1.4.1)\n","Requirement already satisfied: scikit-learn\u003e=0.22.1 in /usr/local/lib/python3.7/dist-packages (from prince-\u003e-r paquetes.txt (line 4)) (0.22.2.post1)\n","Requirement already satisfied: wheel\u003e=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (0.36.2)\n","Requirement already satisfied: joblib\u003e=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (1.0.1)\n","Requirement already satisfied: jinja2\u003e=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (2.11.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (2.7.3)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (0.16.0)\n","Collecting funcy\n","  Downloading https://files.pythonhosted.org/packages/44/52/5cf7401456a461e4b481650dfb8279bc000f31a011d0918904f86e755947/funcy-1.16-py2.py3-none-any.whl\n","Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore\u003c1.21.0,\u003e=1.20.111-\u003eboto3-\u003e-r paquetes.txt (line 1)) (2.8.1)\n","Collecting urllib3\u003c1.27,\u003e=1.25.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/64/43575537846896abac0b15c3e5ac678d787a4021e906703f1766bfb8ea11/urllib3-1.26.6-py2.py3-none-any.whl (138kB)\n","\u001b[K     |████████████████████████████████| 143kB 25.3MB/s \n","\u001b[?25hRequirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.0.2-\u003eprince-\u003e-r paquetes.txt (line 4)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.0.2-\u003eprince-\u003e-r paquetes.txt (line 4)) (2.4.7)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.0.2-\u003eprince-\u003e-r paquetes.txt (line 4)) (1.3.1)\n","Requirement already satisfied: pytz\u003e=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=1.0.3-\u003eprince-\u003e-r paquetes.txt (line 4)) (2018.9)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2\u003e=2.7.2-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (2.0.1)\n","Requirement already satisfied: py\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (1.10.0)\n","Requirement already satisfied: six\u003e=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (1.15.0)\n","Requirement already satisfied: attrs\u003e=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (21.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (57.0.0)\n","Requirement already satisfied: more-itertools\u003e=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (8.8.0)\n","Requirement already satisfied: pluggy\u003c0.8,\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (0.7.1)\n","Requirement already satisfied: atomicwrites\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-\u003epyLDAvis==2.1.2-\u003e-r paquetes.txt (line 5)) (1.4.0)\n","Building wheels for collected packages: pyodbc, pyLDAvis, botocore\n","  Building wheel for pyodbc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyodbc: filename=pyodbc-4.0.31-cp37-cp37m-linux_x86_64.whl size=286759 sha256=f1a7e5915e1f91784acd3015ae20fd5fa1b1b1fe28ce35049d0fb4d202e20b8d\n","  Stored in directory: /root/.cache/pip/wheels/0f/bb/99/22183f16f9e567064147a31b895c45f1ac0b1362e90e6ece01\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97738 sha256=2a53105c7028ade068db5df2e40ce1c05284c75d8ad5d1dda059774473001b5e\n","  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n","  Building wheel for botocore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for botocore: filename=botocore-1.20.111-py2.py3-none-any.whl size=7695032 sha256=4f86434a589c22e78d1ad03c9ab5d66075c597701b2122b0a3cb41bc69ea029c\n","  Stored in directory: /root/.cache/pip/wheels/1e/5f/dd/9021b3f78dc76c95f97ea9cd1798aa6da9bc1a61fe7d1bb9fa\n","Successfully built pyodbc pyLDAvis botocore\n","\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1, but you'll have urllib3 1.26.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3, es-lemmatizer, pyodbc, prince, funcy, pyLDAvis\n","  Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.17.111 botocore-1.20.111 es-lemmatizer-0.2.1 funcy-1.16 jmespath-0.10.0 prince-0.7.1 pyLDAvis-2.1.2 pyodbc-4.0.31 s3transfer-0.4.2 urllib3-1.26.6\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["urllib3"]}}},"metadata":{},"output_type":"display_data"}],"source":["#@markdown instalacion paqutes\n","package = ['boto3','es_lemmatizer','pyodbc','prince',\n","           'pyLDAvis==2.1.2','gensim']\n","f= open(\"paquetes.txt\",\"w+\")\n","\n","def install_txt(package):\n","  f= open(\"paquetes.txt\",\"w+\")\n","  for i in package:\n","    print(i)\n","    \n","\n","    try:\n","        __import__(i)\n","        print('ya instalado {}'.format(i))\n","    except ImportError:\n","        # main(['install', package]) \n","        f.write(\"{} \\r\\n\".format(i))\n","        print('No instalado {}'.format(i))\n","\n","\n","install_txt(package)\n","!pip install -r paquetes.txt  "]},{"cell_type":"markdown","metadata":{"id":"Vr5LKaw2DeDj"},"source":["#**Librerias**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"q6E-NzJiDeDk"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n","don't seem to have the spaCy package itself installed (maybe because you've\n","built from source?), so installing the model dependencies would cause spaCy to\n","be downloaded, which probably isn't what you want. If the model package has\n","other dependencies, you'll have to install them manually.\u001b[0m\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('es_core_news_md')\n"]}],"source":["#Librerias\n","import re\n","import numpy as np\n","import pandas as pd\n","import pandas_profiling\n","from pathlib import Path\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import nltk\n","nltk.download('stopwords')\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","import unicodedata\n","from es_lemmatizer import lemmatize\n","import spacy\n","import re\n","import pyodbc\n","import prince\n","import os\n","from sklearn.feature_extraction.text import CountVectorizer\n","import plotly.express as px\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from scipy import stats\n","import re #expresiones regulares\n","import itertools\n","from collections import Counter \n","import seaborn as sns\n","import matplotlib as mpl\n","plt.style.use('ggplot')\n","sns.set_context(\"talk\")\n","import datetime\n","import dateutil\n","\n","#Librerias de visualizacion dinámica\n","import plotly.express as px\n","from PIL import Image\n","from wordcloud import WordCloud\n","from pprint import pprint #Manipulacion de datos\n","\n","#LDA MODEL FOR OBSERVACIONES\n","#quitar mas profundamente stop_words\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","import unicodedata\n","import tqdm\n","import spacy.cli\n","from spacy.lang.es.stop_words import STOP_WORDS \n","#descargamos los modelos\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('stopwords')\n","spacy.cli.download(\"es_core_news_md\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E_N2blfixB9i"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning:\n","\n","Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","\n"]}],"source":["#@title Librerías modelo de LDA\n","## APLLY LDA MODEL TO OBSERVACIONES\n","#Gensim para modelado de temas, indexación de documentos y recuperación de similitudes con grandes corpus\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","#Spacy para la lemmatization\n","import spacy\n","# Herramientas de graficado\n","import pyLDAvis\n","import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# Habilitado de logging para gensim (opcional)\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n","import warnings\n","warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7eRMw4_37oQi"},"outputs":[],"source":["#@title Librerías  modelo de Word2Vec\n","#word2vec modelo de EMBEDDING\n","#word2vec\n","from gensim.models import word2vec\n","import multiprocessing\n","from gensim.models import Word2Vec\n","from collections import defaultdict \n","import seaborn as sns\n","sns.set_style(\"darkgrid\")\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from scipy import stats\n","from pylab import rcParams\n","from google.colab import widgets"]},{"cell_type":"markdown","metadata":{"id":"J3jU_oc5DeDm"},"source":["#**Rutas**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SjVTmukVHJ15"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["#Rutas en Drive colab\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoTjmiCxlS7m"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRQ7foEmHTnH"},"outputs":[],"source":["Campaña_P  = \"/content/Modelo Analítico PCP - Beneficio Educativo.xlsx\"\n","nombres = \"/content/drive/MyDrive/Ecopetro/Estefania/nombres_apellidos.txt\"\n","# Esta es la ruta donde tengo los nombres: Juan Chacón\n","# nombres = \"/content/drive/MyDrive/Américas BPS/Ecopetro/Estefania/nombres_apellidos.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHAq2At371gc"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Ecopetro/ECOPETROL_P/2. Solicitud de educación inclusiva')\n","# Esta es la dirección en el que yo tengo los archivos\n","# os.chdir('/content/drive/MyDrive/Américas BPS/Ecopetro/ECOPETROL_P/2. Solicitud de educación inclusiva')\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XC5Yr36Z8A1g"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"64b4yJA8DeDn"},"source":["#**Funciones**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gbzpV7UDeDn"},"outputs":[],"source":["# CONEXION A SERVIDOR ###################################################################################\n","# def Conexion():\n","#     server = '172.27.48.148'\n","#     #database = 'bdcmp_NuevaEpsCrm' \n","#     username = 'CienciaDatos' \n","#     password = 'C13nc1aD47*5'  \n","#     cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';UID='+username+';PWD='+ password)\n","#     cursor = cnxn.cursor()\n","#     return cnxn\n","\n","\n","# DEFINICION DE PICOS Y FECHAS ##################################################################################\n","##Se definen fechas para realizar los respectivos filtros \n","##cuando se extraiga la información correspondiente desde el servidor\n","\n","def fechas(df_Tipo_OF):\n","    df_OF = df_Tipo_OF.astype(str).replace('NaT','20200101')\n","#######Operaciones financieras\n","    df_OF ['fecha_ini_1'] = df_OF['Picos de demanda  1'].replace(regex=r'-', value = '').replace('0','20200101')\n","    df_OF ['fecha_fin_1'] = ['20210401','20210301','20210301','20210301', '20210301', '20210301', '20210401']\n","    df_OF ['fecha_ini_2'] = df_OF['Picos de demanda  2'].replace(regex=r'-', value = '').replace('0','20200101')\n","    df_OF ['fecha_fin_2'] = ['20210501', '20210501', '20200101', '20210501', '20200101', '20210501', '20201001']\n","    df_OF ['fecha_ini_3'] = df_OF['Picos de demanda  3'].replace(regex=r'-', value = '').replace('0','20200101')\n","    df_OF ['fecha_fin_3'] = ['20210601', '20200101', '20200101', '20210601', '20200101', '20200101', '20210101']\n","    \n","    return df_OF  \n","\n","## EXTRACCIÓN ########################################################################################################\n","\n","def Extraccion(df_fechas, df_General, picos):\n","    #df_fechas : Nombre de la tabla con las tipologias seleccionadas y las fechas definidas correspondientes (con la función de fechas)\n","    #df_general : Nombre de tabla para cada campaña sin las fechas definidas \n","    #picos : Número máximo de picos que se analizan [en OF son 2, en P son 3]\n","    \n","    query_base = \"SELECT * FROM [172.27.90.50].[Ecopetrol_Reporting].[dbo].[Tbl_Ecopetrol_ResultadoBackOffice] \"\n","    \n","    i = 0 \n","    cnxn = Conexion()\n","    query = \"WHERE ([fgs_Nivel3] = '\"+ df_fechas['Nivel 3'][i] + \"') AND ([fgs_FechaInicio] BETWEEN '\" + df_fechas['fecha_ini_1'][i]+ \"' AND '\" + df_fechas['fecha_fin_1'][i] +\"')\"  \n","    query_completa = query_base + query\n","    df_inicial = pd.read_sql(query_completa, cnxn)\n","\n","    for j in range(1,picos+1):\n","        df_General[\"Datos_Pico_\"+str(j)] = 0\n","        for i in range(0,len(df_fechas['Campaña'])):\n","            cnxn = Conexion()\n","            query = \"WHERE ([fgs_Nivel3] = '\"+ df_fechas['Nivel 3'][i] + \"') AND ([fgs_FechaInicio] BETWEEN '\" + df_fechas[\"fecha_ini_\"+str(j)][i]+ \"' AND '\" + df_fechas[\"fecha_fin_\"+str(j)][i] +\"')\"  \n","            query_completa = query_base + query\n","            df = pd.read_sql(query_completa, cnxn)\n","            df_total = pd.concat([df_inicial,df]).drop_duplicates()\n","            df_inicial = df_total\n","            df_General[\"Datos_Pico_\"+str(j)][i] = df.shape[0] \n","    print('Total de registros:', df_inicial.shape[0])\n","    print('Total de columnas:', df_inicial.shape[1])\n","            \n","    return df_total,df_General                 \n","\n","# VALORES FALTANTES ###############################################################################################################################################################################################\n","\n","def Valores_Faltantes(df, porcentaje):\n","    Faltantes = df.isnull().sum()\n","    df_Faltantes = pd.DataFrame({'Nombre_Columnas':Faltantes.index, 'Total_Vacios':Faltantes.values})#[df_Faltantes['Total_Vacios'] != 0]\n","    df_Faltantes = df_Faltantes[df_Faltantes['Total_Vacios'] != 0]\n","\n","    df_Faltantes['Porcentaje_vacio'] = ((df_Faltantes['Total_Vacios']/df.shape[0])*100).round(2)\n","    df_Faltantes['Columna_vacia'] = 'NO'\n","    df_Faltantes.loc[df_Faltantes['Total_Vacios'] == df.shape[0], 'Columna_vacia'] = 'SI'\n","    \n","    print('Total columnas con valores faltantes :', df_Faltantes.shape[0])\n","    print('Total columnas vacias :', df_Faltantes[df_Faltantes['Columna_vacia'] == 'SI'].Columna_vacia.count())\n","    print('Total columnas no vacias con mas del ' + str(porcentaje) + '% de valores faltantes :', df_Faltantes[(df_Faltantes['Porcentaje_vacio'] \u003e= porcentaje) \u0026 (df_Faltantes['Columna_vacia'] == 'NO') ].Columna_vacia.count())\n","    #print('Total columnas no vacias con menos del ' + str(porcentaje) + '% de valores faltantes :', df_Faltantes[(df_Faltantes['Porcentaje_vacio'] \u003e= porcentaje) \u0026 (df_Faltantes['Columna_vacia'] == 'NO') ].Columna_vacia.count())\n","    \n","    return df_Faltantes\n","\n","# ELIMINAR COLUMNAS##############################################################################################\n","\n","#para eliminar columnas seleccionadas\n","def Eliminar_columnas(df, lista_columnas):\n","    df_drop = df.drop(columns = lista_columnas)\n","    print ('Cantidad de columnas iniciales : ', df.shape[1])\n","    print ('Cantidad despues de eliminar : ', df_drop.shape[1] )\n","    return df_drop\n","### elimincacion 100 valores nulos, cardinalida100, cardinalidad 1\n","def eliminacion_columnas(tabla):\n","  cardinalidad100 = tabla[tabla['% de valores diferentes']==100]\n","  cardinalidad1 = tabla[tabla['# de valores diferentes']==1]\n","  vacios = tabla[tabla['% de datos nulos']==100]\n","  variables_eliminar=vacios.index.tolist()+cardinalidad1.index.tolist()+cardinalidad100.index.tolist()# no aportan informacion\n","  print('numero de variables a eliminar : {}'.format(len(variables_eliminar)))\n","  return variables_eliminar\n","# columnas duplicadas\n","def columnas_dup(df):\n","    df_dup = df.T.duplicated().T\n","    return df_dup\n","#Para eliminar columnas duplicadas\n","def columnas_nodup(df):\n","    print ('Total de columnas antes de eliminar dup: ', df.shape[1])\n","    df_sindup = df.T.drop_duplicates().T\n","    print ('Total de columnas duplicadas : ', df.shape[1] - df_sindup.shape[1])\n","    print ('Total de columnas no duplicadas:', df_sindup.shape[1])\n","    return df_sindup    \n","\n","def mytable(data):\n","    nulls = pd.DataFrame(data.isnull().sum()/data.shape[0]*100, columns=['% de datos nulos'])\n","    ceros = pd.DataFrame((data==0).sum()/data.shape[0]*100, columns=['% de ceros'])\n","    tipo = pd.DataFrame(data.dtypes,columns=['Tipo de datos'])\n","    val = pd.DataFrame(data.nunique(),columns=['# de valores diferentes'])\n","    valpor = pd.DataFrame(data.nunique()/data.shape[0]*100,columns=['% de valores diferentes'])\n","    mis_val_table = pd.concat([nulls, ceros,tipo, val, valpor], axis=1)\n","    return mis_val_table\n","\n","def creacion_tabla(x):\n","  tabla_x = mytable(x)\n","  print(x.shape)\n","  return tabla_x\n","\n","\n","def nonum(x):\n"," try:\n","   int(x)\n","   sal = False\n"," except:\n","    sal = True\n"," return sal \n","\n","\n","def isnum(x):\n"," try:\n","   int(x)\n","   sal = True\n"," except:\n","    sal = False\n"," return sal \n","\n","def solo_num(x):\n","  try:\n","    sal = float(x)\n","  except:\n","    sal = np.nan\n","  return sal \n","def creacion_tabla(x):\n","  tabla_x = mytable(x)\n","  print(x.shape)\n","  return tabla_x\n","\n","\n","\n","\n","\n","#esta funcion quita stop words de cada una de las filas\n","def text_rows(texto, StopWords):\n","    texto = texto.split()\n","    resultwords  = [word for word in texto if word not in StopWords]\n","    texto = ' '.join(resultwords)\n","    return texto\n","\n","\n","def text (column, dic):\n","    column = column.astype(str)\n","    texto_base = \" \".join(motivo for motivo in column)\n","    text = (unicodedata.normalize('NFKD', texto_base).encode('ascii', 'ignore').decode('utf-8', 'ignore').lower())\n","    text = re.sub(r\"\\bmedicas\\b\",\"medica\",text)\n","    for i, j in dic.items():\n","        text = text.replace(i, j)\n","        \n","    return text\n","    \n","def listas (column, dic):\n","    Lista = []\n","    column = columna.astype(str)\n","    for motivo in column:\n","        inicial_answer = str(motivo)\n","        for i, j in dic.items():\n","            ini_answer = inicial_answer.replace(i, j)\n","        answer = (unicodedata.normalize('NFKD', ini_answer).encode('ascii', 'ignore').decode('utf-8', 'ignore').lower())\n","        answer = re.sub(r\"\\bmedicas\\b\",\"medica\",answer)\n","        answer = re.sub(r\"\\bmedicamentos\\b\",\"medicamento\",answer)\n","        answer = re.sub(r'[^\\w\\s]','', answer).split()\n","        words = [word for word in answer if word not in stop_words]\n","        Lista.append(words)\n","    return Lista\n","    \n","#crea nube de palabras\n","def wordcloud(text, nombre, stopWords):\n","    # wordcloud = WordCloud(stopwords=stop_words+stopWords, background_color=\"white\", width=1600, height=800,min_word_length =3 ).generate(text)\n","    wordcloud = WordCloud(stopwords=stop_words+stopWords, background_color=\"white\", width=1600, height=800).generate(text)\n","    plt.figure( figsize=(15,10) )\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.show()\n","    plt.savefig(\"word_cloud_\"+str(nombre)+\".png\") #dpi = 300)    \n","\n","#crea n-gramas   \n","def ngrams(text, n, top,stopWords):\n","    #wnl = nltk.stem.WordNetLemmatizer()\n","    texto = re.sub(r'[^\\w\\s]','', text).split()\n","    stopwords = stop_words + stopWords\n","    words = [word for word in texto if word not in stopwords]\n","    \n","    n_grams_series = ((pd.Series(nltk.ngrams(words,n)).value_counts())[:top])\n","    plot = n_grams_series.sort_values().plot.barh(color =\"tomato\", width = 0.6, figsize = (20,18), edgecolor='grey')   #color=(0.2, 0.4, 0.6, 0.6)\n","    plot.set_xlabel('Frecuencia',  fontname=\"Calibri\", fontsize=35)\n","    plot.set_title('TOP ' + str(top) +' de '+str(n)+'-GRAMAS QUE APARECEN CON MAYOR FRECUENCIA', fontname=\"Calibri\", fontsize=40)\n","    plot.title.set_position([.5, 1.05])\n","    for tick in  plot.get_xticklabels():\n","        tick.set_fontname(\"Calibri\")\n","        tick.set_fontsize(35)\n","    for tick in  plot.get_yticklabels():\n","        tick.set_fontname(\"Calibri\")\n","        tick.set_fontsize(35)\n","    #plt.savefig(\"graph.png\") #dpi = 300)\n","    plt.figure( figsize=(15,10) )\n","    #plt.show()\n","    return n_grams_series\n","    #if n == 1 :\n","    #    return n_grams_series.to_frame().reset_index().rename(columns = {'index':'palabras', 0:'Conteo'})\n","    #elif n == 2:\n","    #    return n_grams_series.to_frame().reset_index().rename(columns = {'index':'Bigramas', 0:'Conteo'})\n","    #elif n \u003e 2:\n","    #    return n_grams_series.to_frame().reset_index().rename(columns = {'index':'gramas', 0:'Conteo'})\n","\n","def wordcloud_jose(x):\n","  lista_mensajeuser=[]\n","  \n","  long_string=''.join(x)\n","  sal = pd.Series(long_string.split(' ')).value_counts()\n","  saldic = sal.to_dict()\n","  #Creacion lista stop words\n","  wordcloud = WordCloud(background_color=\"white\",min_font_size=5, max_font_size=150, max_words=600, contour_width=50,\n","                        contour_color='steelblue', margin=15, stopwords=StopWords_total,width=1600, height=800)\n","  \n","  wordcloud2 = WordCloud(background_color=\"white\",min_font_size=5, max_font_size=150, max_words=20, contour_width=50,\n","                      contour_color='steelblue', margin=15, stopwords=StopWords_total,width=1600, height=800)\n","  #Crear el word cloud\n","  #wordcloud.generate(long_string)\n","  wordcloud.generate_from_frequencies(saldic)\n","  wordcloud2.generate_from_frequencies(saldic)\n","  # wordcloud.generate(saldic)\n","  #Visualizar el word cloud\n","  plt.figure( figsize=(15,10) )\n","  \n","  plt.imshow(wordcloud, interpolation='bilinear')\n","  plt.axis(\"off\")\n","  plt.show()\n","  \n","  plt.figure( figsize=(15,10) )\n","  plt.imshow(wordcloud2, interpolation='bilinear')\n","  plt.axis(\"off\")\n","  plt.show()\n","\n","\n","def plot_10_most_common_words(count_data, count_vectorizer):\n","   #Grafico de frecuencias\n","    words = count_vectorizer.get_feature_names()\n","    total_counts = np.zeros(len(words))\n","    for t in count_data:\n","        total_counts+=t.toarray()[0]\n","    \n","    count_dict = (zip(words, total_counts))\n","    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:15]\n","    words = [w[0] for w in count_dict]\n","    counts = [w[1] for w in count_dict]\n","    x_pos = np.arange(len(words)) \n","    \n","    fig = px.bar( x=words, y=counts, color_discrete_sequence=[\"#25427B\",\"#33BBFF\",\"#EBECED\"], title='TÉRMINOS MÁS COMUNES EN:'+' '+segmento,  labels=dict(x=\"Palabra Clave\", y=\"Frecuencia\", color=\"Place\")) #COLORES DE ECOPETROL, PUEDES VARIAR LOS COLORES\n","    fig.write_html(segmento+\"_.html\")\n","    fig.show()\n","    return words,counts\n","\n","\n","\n","def sent_to_words(sentences):\n","      for sentence in sentences:\n","          yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n","\n","def adjustment_all(text, dic):\n","    for i, j in dic.items():\n","        text = text.replace(i, j)\n","    return text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVnhnWl9xKAX"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6solbCd8BhO"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"1iVAKm2ADeDs"},"source":["# **EXTRACCIÓN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrDiVpwxy6lh"},"outputs":[],"source":["# observador_personal  = '/content/drive/MyDrive/Ecopetro/Estefania/Observaciones_LimpiezaInicial_Personal.csv'\n","observador_personal  ='/content/drive/MyDrive/Ecopetro/ECOPETROL_P/Observaciones_LimpiezaInicial_Personal.csv'\n","#Este es mi ruta: Juan Chacón\n","# observador_personal  ='/content/drive/MyDrive/Américas BPS/Ecopetro/ECOPETROL_P/Observaciones_LimpiezaInicial_Personal.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"botx2aohy7Nh"},"outputs":[],"source":["beneficios = pd.read_csv(observador_personal)\n","beneficios.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXPfDHqr1LbS"},"outputs":[],"source":["mytable(beneficios[['fgs_IdGestion', 'fgs_Nivel3', 'ObservacionCreacion_limpieza1',\n","'ObservacionSolucion_limpieza1', 'ObservacionEscalamientoN2_limpieza1']])"]},{"cell_type":"markdown","metadata":{"id":"yzY6YoaOs2IE"},"source":["## **GRUPOS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39nPvgLhs2IF"},"outputs":[],"source":["GRUPOS = ['1. Legalización','1. Legalización','2. Solicitud de educación inclusiva'\n","          ,'3. Anticipo','3. Anticipo','3. Anticipo','3. Anticipo'\n","          ,'4. Consulta','4. Consulta','4. Consulta','4. Consulta','4. Consulta'\n","          ,'4. Consulta','4. Consulta','5. Soporte Plataforma'\n","          ,'6. Reintegro','6. Reintegro','7. Inscripción','7. Inscripción','7. Inscripción']\n","\n","NIVELES = ['Legal / Reconocimiento Pensionados (Incluye Sustitutos)'\n","           ,'Legal / Reconocimiento Trabajadores (Incluye Bachiller ECP)'\n","           ,'Solicitud de educación inclusiva'\n","           ,'Anticipo Pensionados Educación Inclusiva','Anticipo Trabajadores Educación Inclusiva'\n","            ,'Anticipo Pensionados (Incluye Sustitutos)','Anticipo Trabajadores (Incluye BXC)'\n","            ,'Consulta Bachiller Ecopetrol (Incapacidad médica y/o Fuerza Mayor)'\n","            ,'Consulta de información relacionadas con reembolsos'\n","            ,'Consulta especialista beneficios atención PQRS'#quitar\n","            ,'Consulta sobre liquidación/pago préstamo para educación'#liquidacion pago prestamo\n","            ,'Consulta Titulares Beneficio Eductivo - Desvinculados'#\n","            ,'Consultas Titulares Beneficio Educativo (No Incluye Bachiller ECP)'\n","            ,'Solicitud / Consulta / revisión pagos colegios propiedad de Ecopetrol S.A.'\n","            ,'Soporte Plataforma para Instituciones Educativas vía TIB'\n","            ,'Reintegro de dinero Pensionados Educación (Incluye sustitutos y Auto)'\n","            ,'Reintegro de dinero trabajadores Educación (Incluye Bachiller ECP)'\n","            ,'Inscripción casos especiales comité de educación convencional activos'\n","            ,'Inscripción casos especiales comité educación convencional pensionados'\n","            ,'Inscripción solicitud becas Comité de Educación Convencional Activos']#inscripcion convencional (activos, pensionados, )\n","            #becas  convencional activos\n","\n","#trabajadores, pensionados , sustituto sin incluye\n","dic_grupos = dict(zip(NIVELES,GRUPOS))\n","\n","beneficios['GRUPOS'] = beneficios['fgs_Nivel3']\n","beneficios['GRUPOS'].replace(dic_grupos, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIAUhmMY_UQS"},"outputs":[],"source":["interes = beneficios[[ 'fgs_IdGestion','GRUPOS', 'ObservacionCreacion_limpieza1',\n","'ObservacionSolucion_limpieza1', 'ObservacionEscalamientoN2_limpieza1']].copy()\n","\n","interes['ObservacionCreacion']=beneficios['ObservacionCreacion_limpieza1']#+\" \"+beneficios['fgs_Nivel3']\n","## no usar la tipologia\n","\n","# interes['ObservacionSolucion_+_n2'] = beneficios['ObservacionSolucion_limpieza1']+' '+beneficios['ObservacionEscalamientoN2_limpieza1'].map(lambda x : x.replace('vacio',''))# aqui el problema\n","interes['ObservacionSolucion_+_n2'] = beneficios['ObservacionSolucion_limpieza1']+' '+beneficios['ObservacionEscalamientoN2_limpieza1']#.map(lambda x : re.sub(r'vacio', '', x))\n","# interes['ObservacionSolucion_+_n2'] = beneficios['ObservacionSolucion_limpieza1']+' '+beneficios['ObservacionEscalamientoN2_limpieza1'].str.replace('vacio', '',regex = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_EmaemLljNZ"},"outputs":[],"source":["# texto = 'obserne'\n","# mapa = interes['ObservacionSolucion_+_n2'].astype(str).map(lambda x: x.find(texto))!=-1\n","# pd.DataFrame(interes[['ObservacionSolucion_+_n2','fgs_IdGestion'\t]][mapa]).set_index('ObservacionSolucion_+_n2').head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veoOsFpvlU3J"},"outputs":[],"source":["interes[interes['fgs_IdGestion'] ==13910437][['ObservacionEscalamientoN2_limpieza1']].set_index('ObservacionEscalamientoN2_limpieza1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppXHc3KAf8_r"},"outputs":[],"source":["texto_total = interes['ObservacionCreacion_limpieza1'].astype(str).sum()\n","len(texto_total.split(' '))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d01QJni0gqyQ"},"outputs":[],"source":["texto_total = interes['ObservacionSolucion_+_n2'].astype(str).sum()\n","len(texto_total.split(' '))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lvk_4MTpBeN7"},"outputs":[],"source":["interes.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBdgsR-lBstL"},"outputs":[],"source":["interes = interes[['fgs_IdGestion','GRUPOS', 'ObservacionCreacion_limpieza1',\n","       'ObservacionSolucion_limpieza1',\n","       'ObservacionCreacion', 'ObservacionSolucion_+_n2',\n","       'ObservacionEscalamientoN2_limpieza1']].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzUh7x8yCDF0"},"outputs":[],"source":["\n","# for i in interes.columns:\n","#   display(interes[i].value_counts())\n","#   print('_'*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgJrSI1hDRmo"},"outputs":[],"source":["\n","for i in interes.columns:\n","  display(pd.DataFrame(interes[i].value_counts()))\n","  print('_'*100)\n","\n","  # si hay cambios"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQyMEzVdAWD4"},"outputs":[],"source":["original =interes.copy()"]},{"cell_type":"markdown","metadata":{"id":"Lb5LH48D0qL_"},"source":["## Limpieza nivel general"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7eVJ1kj9-0l"},"outputs":[],"source":["# crecion columna = creacion + nivel 3\n"]},{"cell_type":"markdown","metadata":{"id":"v9iocPV76KKA"},"source":["## Pipe line de limpieza"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8MZ0_ql6PUz"},"outputs":[],"source":["pd.DataFrame(interes['ObservacionCreacion'].value_counts()).head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynhqZZgDzRHp"},"outputs":[],"source":["# trabajadores, pensionados , sustitutos.(incuyecon )\n","#Pensionados (Incluye Sustitutos) solicita informacion dejar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9RMlFmVqz0Q"},"outputs":[],"source":["pd.DataFrame(interes['ObservacionSolucion_+_n2'].value_counts()).head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0rzBjD7edDd"},"outputs":[],"source":["original.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMck2EHzi0S-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ToeoF_vr67bo"},"outputs":[],"source":["# interes, original = eliminacion_stopwords()\n","#Eliminamos los espacios al inicio y al final de cada strig\n","\n","interes['ObservacionSolucion_+_n2']=interes['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.replace('(','') )\n","interes['ObservacionCreacion']=interes['ObservacionCreacion'].astype(str).apply(lambda x: x.replace('(','') )\n","\n","interes['ObservacionSolucion_+_n2']=interes['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.replace(')','') )\n","interes['ObservacionCreacion']=interes['ObservacionCreacion'].astype(str).apply(lambda x: x.replace(')','') )\n","\n","interes['ObservacionSolucion_+_n2']=interes['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.replace('/','') )\n","interes['ObservacionCreacion']=interes['ObservacionCreacion'].astype(str).apply(lambda x: x.replace('/','') )\n","\n","interes['ObservacionSolucion_+_n2']=interes['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.replace('  ',' ') )\n","interes['ObservacionCreacion']=interes['ObservacionCreacion'].astype(str).apply(lambda x: x.replace('  ',' ') )\n","\n","interes['ObservacionSolucion_+_n2']=interes['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.strip() if type(x)==str else x)\n","interes['ObservacionCreacion']=interes['ObservacionCreacion'].astype(str).apply(lambda x: x.strip() if type(x)==str else x)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOpGUWUZYAYh"},"outputs":[],"source":["print(\n","sum(interes['ObservacionSolucion_+_n2'].astype(str).map(lambda x : len(x.split(' ')))),\n","sum(interes['ObservacionCreacion'].astype(str).map(lambda x : len(x.split(' ')))),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAqG0gvJDoyh"},"outputs":[],"source":["stop_words=[]\n","guiones =[]\n","\n","#Guiones en bruto\n","guiones+=['gestion su caso se escalo al area encargada para su tramite con las siguientes observaciones',\n","          'se envia respuesta adjunta por correo oficina virtual desde servicios compartidos queremos solucionar todos tus requerimientos de acuerdo con su solicitud de',\n","          'requisitos reconocimiento plan educacional estudios superiores formulario diligenciado y firmado por el titular y por la institucion en caso de que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico falta como se le indica debe adjuntar la certificacion con la informacion solicitada semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico ya que el formato no lo firma el plantel educativo la certificacion que presenta faltan dados solicitados uno de los siguientes documentos a original de recibo de pago que especifique el valor ordinario de matricula de la institucion con los respectivos sellos de cancelado del banco escanearlo mejor b en caso de pagos electronicos debe presentar soporte de la transferencia donde se observe que la transaccion fue exitosa o aceptada anexando documento de la institucion educativa que detalle el valor ordinario de matricula c certificacion emitida por la institucion donde indique el valor de matricula ordinaria cancelada para solicitudes de primer semestre cambio de carrera o universidad debe anexar programa academico pensum malla curricular en caso de beca postgrado hijos para personal beneficiario de la convencion adjuntar copia del diploma o acta de grado para bachilleres ecopetrol anexar certificado de promedio certificado de la carrera nota en caso de presentar alguna restriccion en el aplicativo se agradece enviar toda la documentacion requerida para este tramite en un solo archivo pdf peso maximo mb junto con el print del error presentado donde agrupe la totalidad de los soportes exigidos por favor no enviar archivos separados al correo electronico de oficina virtual de personal con el fin de brindarle toda la asistencia necesaria a su requerimiento oficinavirtualdepersonal ecopetrol com co vacio',\n","        \n","          'le informamos que para proceder a tramitar la solicitud debe enviar nuevamente todos los documentos junto con el pantallazo del error requisitos reconocimiento plan educacional estudios superiores formulario diligenciado y firmado por el titular y por la institucion en caso de que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico',\n","          'desde servicios compartidos mantenemos una comunicacion y oportuna con nuestros clientes por eso a continuacion encontrara toda informacion relacionada con',\n","          'ingresa a autoservicioingrese usuario y contrasena redseleccione pestana beneficio modulo beneficio educativoseleccione su beneficiario dando clic opcion crear solicitud y continue proceso caso que presente inconvenientes con ingreso a autoservicio tomar captura pantalla error y enviar al buzon habilitado oficinavirtualdepersonal com co con documentos pertinentes por este medio un solo archivo formulario diligenciado y firmado por titular y por caso que no diligencie formulario debera presentar certificacion avance que conste informacion formulario semestre a cursar periodo lectivo valor por matricula ordinaria programa academico uno siguientes documentos a original recibo pago que especifique valor ordinario matricula con respectivos sellos banco caso pagos electronicos debe presentar soporte transferencia donde se observe que transaccion fue exitosa o aceptada anexando documento que detalle valor ordinario matriculac certificacion emitida por donde indique valor matricula ordinaria cancelada solicitudes primer semestre cambio o certificado admision o carta aceptacion por parte debe anexar programa academico pensum malla curricular',\n","          'dando alcance al correo precedente nos permitimos informarle que',\n","          'new party has joined the session',\n","          'sala chat breve sera atendido'\n","          'su interaccion quedo registrada bajo el numero lo invito a calificar nuestro servicio en la siguiente encuesta '\n","          ]\n","\n","# Guiones que pueden estar modificados\n","guiones+=['nos permitimos informarle que en atencion a su' \n","          'puede realizarlo ingresa a autoservicioingrese usuario y contrasena de redseleccione la pestana beneficio educativo modulo beneficio educativoseleccione el beneficiario dando clic en la opcion crear solicitud y continue el procesoen caso que presente inconvenientes con el ingreso a autoservicio tomar captura de pantalla del error y enviar por el buzon habilitado oficinavirtualdepersonal ecopetrol com co con los documentos pertinentes en un solo archivo pdf formulario diligenciado y firmado por el titular y por la institucion en caso que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico uno de los siguientes documentos a original de recibo de pago que especifique el valor ordinario de matricula de la institucion con los respectivos sellos de cancelado del banco b en caso de pagos electronicos debe presentar soporte de la transferencia donde se observe que la transaccion fue exitosa o aceptada anexando documento de la institucion educativa que detalle el valor ordinario de matriculac certificacion emitida por la institucion donde indique el valor de matricula ordinaria cancelada para solicitudes de primer semestre cambio de carrera o universidad certificado de admision o carta de aceptacion por parte de la institucion educativa debe anexar programa academico pensum malla curricular todos los documentos deben ser adjuntados en un solo archivo pdf con peso no mayor a mb vacio',\n","          'puede realizarlo ingresa a autoservicioingrese usuario y contrasena de redseleccione la pestana beneficio educativo modulo beneficio educativoseleccione el beneficiario dando clic en la opcion crear solicitud y continue el procesoen caso que presente inconvenientes con el ingreso a autoservicio tomar captura de pantalla del error y enviar por el buzon habilitado oficinavirtualdepersonal ecopetrol com co con los documentos pertinentes en un solo archivo pdf formulario diligenciado y firmado por el titular y por la institucion en caso que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico uno de los siguientes documentos a original de recibo de pago que especifique el valor ordinario de matricula de la institucion con los respectivos sellos de cancelado del banco b en caso de pagos electronicos debe presentar soporte de la transferencia donde se observe que la transaccion fue exitosa o aceptada anexando documento de la institucion educativa que detalle el valor ordinario de matriculac certificacion emitida por la institucion donde indique el valor de matricula ordinaria cancelada para solicitudes de primer semestre cambio de carrera o universidad certificado de admision o carta de aceptacion por parte de la institucion educativa debe anexar programa academico pensum malla curricular todos los documentos deben ser adjuntados en un solo archivo pdf con peso no mayor a mb vacio',\n","          'ingresa a autoservicioingrese usuario y contrasena de redseleccione la pestana beneficio educativo modulo beneficio educativoseleccione su beneficiario dando clic en la opcion crear solicitud y continue el proceso en caso que presente inconvenientes con el ingreso a autoservicio tomar captura de pantalla del error y enviar al buzon habilitado oficinavirtualdepersonal ecopetrol com co con los documentos pertinentes por este medio en un solo archivo formulario diligenciado y firmado por el titular y por la institucion en caso que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico uno de los siguientes documentos a original de recibo de pago que especifique el valor ordinario de matricula de la institucion con los respectivos sellos de cancelado del banco b en caso de pagos electronicos debe presentar soporte de la transferencia donde se observe que la transaccion fue exitosa o aceptada anexando documento de la institucion educativa que detalle el valor ordinario de matriculac certificacion emitida por la institucion donde indique el valor de matricula ordinaria cancelada para solicitudes de primer semestre cambio de carrera o universidad certificado de admision o carta de aceptacion por parte de la institucion educativa debe anexar programa academico pensum malla curricular caida vpn perdida de la llamada vacio',\n","          'formulario diligenciado y firmado por el titular y por la institucion en caso que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico uno de los siguientes documentos a original de recibo de pago que especifique el valor ordinario de matricula de la institucion con los respectivos sellos de cancelado del banco b en caso de pagos electronicos debe presentar soporte de la transferencia donde se observe que la transaccion fue exitosa o aceptada anexando documento de la institucion educativa que detalle el valor ordinario de matricula c certificacion emitida por la institucion donde indique el valor de matricula ordinaria cancelada para solicitudes de primer semestre cambio de carrera o universidad debe anexar programa academico pensum malla curricular vacio',\n","          'cordial saludo en atencion a su comunicado nos permitimos',\n","          'desde servicios compartidos mantenemos una comunicacion clara y oportuna con nuestros clientes por eso a continuacion encontrara toda la informacion relacionada con la solicitud referente a',\n","          'desde servicios compartidos mantenemos una comunicacion clara y oportuna con nuestros clientes',\n","          'sus opiniones son importantes para nosotros con el fin de seguir apuntando nuestros esfuerzos a la mejora de los servicios ofrecidos',\n","          'queremos informarle que desde el contact center disponemos de varios canales de contacto para la atencion de solicitudes relacionadas con servicios al personal',\n","          'requisitos reconocimiento plan educacional estudios superiores formulario diligenciado y firmado por el titular y por la institucion en caso de que la institucion no diligencie el formulario debera presentar certificacion de avance en la que conste la informacion del formulario semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico',\n","          'certificacion con la informacion solicitada semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico ya que el formato no lo firma el plantel educativo falta la certificacion valor cancelado matricula',\n","          'canales telefonicos linea gratuita nacional linea directa bogota',\n","          'canal correo electronico oficinavirtualdepersonal ecopetrol com co',\n","          'dando alcance al correo precedente nos permitimos informarle que para',\n","          'se informa que',\n","          'su caso se escalo al area encargada para su tramite con las siguientes observaciones',\n","          'https gestiondepersonal ecopetrol com co',\n","          'link https gestiondepersonal com co',\n","          'buen dia','oficinavirtual com co','buen dia atencion',\n","          'siguientes observaciones',\n","          'siguientes observaciones',\n","          'caso escalo',\n","          'caso escalado',\n","          'com co',\n","          'desde servicios compartidos queremos solucionar todos tus requerimientos acuerdo',\n","          'valor ordinario',\n","          'buenas tardes',\n","          'buenos dias',\n","          'cordial saludo',\n","          'nos permitimos informarle',\n","          'oficina virtual',\n","          'he tratado',\n","          'quedo atento',\n","          'de manera atenta',\n","          'manera atenta',\n","          'por favor',\n","          'ha sido',\n","          'solicitud recibida registrada bajo numero',\n","          'solicitud recibida registrada bajo numero adjunto',\n","          'recibiras confirmacion estado tu tramite durante tiempo maximo dias contados partir fecha solicitud',\n","          'le recordamos que a la fecha presenta un', \n","           'del periodo lectivo a por el beneficiario a',\n","          'usuario contrasena',\n","          'adjuntando documentacion unico archivo pdf formulario diligenciado firmado diligencie formulario debera certificacion avance conste informacion formulario semestre cursar periodo lectivo valor matricula ordinaria programa academico original recibo pago especifique matricula sellos banco pagos electronicos soporte transaccion exitosa aceptada documento detalle matricula certificacion emitida valor matricula ordinaria cancelada solicitudes primer semestre cambio anexar programa academico pensum malla curricular',\n","          'documento generado por la universidad en donde consta la informacion del semestre a cursar periodo lectivo valor cancelado por matricula ordinaria programa academico recibo de pago en donde se especifica el valor ordinario de matricula de la institucion el respectivo sello de cancelado del banco factura electronica de venta expedida por la universidad libre acta matricula con horario',\n","          'ustedes dicen',\n","          'periodo lectivo',\n","          'bajo numero adjunto segun diferente asegure',\n","          'bajo numero',\n","          'continuacion relacionamos requisitos cuales tambien puede validar respaldo formulario gth formulario diligenciado firmado diligencie formulario debera certificacion avance informacion formulario semestre cursar valor matricula ordinaria programa academico original recibo pago espeficique matricula sellos banco pagos electronicos soporte transaccion exitosa aceptada documento detalle matricula certificacion emitida valor matricula ordinaria cancelada solicitudes primer semestre cambio anexar programa academico pensum malla curricular beca postgrado hijos beneficiario convencion adjuntar copia diploma acta grado bachilleres anexar certificado promedio certificado revisando documentacion enviada falta documento recibo pago especifique matriculauna vez tenga completos enviolos nuevamente todos archivo pdf oficinavirtualdepersonal mencionando inicial muchas hacer uso nuestros servicios invitamos continuar sus solicitudes medio portal servicios compartidos linea unica atencion desde servicios compartidos placer gestionar sus solicitudes agradecemos atencion prestada deseamos feliz dia anexan soportes',\n","          'has left the session',\n","          'medio presente permito muy respetuosamente',\n","          'muy respetuosamente',\n","          'sellos banco pagos electronicos soporte transaccion exitosa aceptada documento detalle',\n","          'alguna falla error aplicativo usted podra envio toda documentacion requerida tramite archivo pdf peso maximo agrupe totalidad soportes exigidos',\n","          'numero cedula sin puntos ni espacios pensionados sustitutos bachilleres colombia embargantes contrasena recuerda deja da click recordar clave sistema envia automaticamente electronico registrado ante trabajadores directos mismo clave red',\n","          'diligenciado firmado diligencie formulario debera certificacion avance informacion formulario semestre cursar valor matricula programa academico original recibo pago especifique matricula matricula certificacion emitida valor matricula cancelada',\n","          'diligencie formulario debera certificacion avance informacion formulario semestre cursar valor matricula programa academico original recibo pago especifique matricula matricula certificacion emitida valor matricula cancelada solicitudes primer semestre cambio anexar programa academico pensum malla curricular',\n","          'diligencie formulario debera certificacion avance informacion formulario semestre cursar valor matricula programa academico original recibo pago especifique matricula matricula certificacion emitida valor matricula cancelada solicitudes primer semestre cambio anexar programa academico pensum malla curricular',\n","          'documentacion requerida tramite archivo pdf peso maximo junto print error presentado agrupe totalidad soportes exigidos envio archivos separados electronico fin brindarle toda asistencia necesaria requerimiento',\n","          'avance informacion formulario semestre cursar valor matricula programa academico original recibo pago especifique matricula matricula certificacion emitida valor matricula cancelada solicitudes primer semestre cambio anexar programa academico',\n","          'certificacion avance informacion formulario semestre cursar valor matricula programa academico',\n","          'firmado diligencie formulario debera certificacion avance informacion formulario semestre cursar',\n","          'recuerde hablo interaccion quedo registrada invito calificar nuestro servicio encuesta',\n","          'identificado cedula ciudadania',\n","          'identificada cedula ciudadania',\n","          't i',\n","          'siempre acostumbrada colaboracion',\n","          'siempre acostumbrada colaboracion medio presente'\n","          'nueva persona incorporo sesion sala chat breve',\n","          'nueva persona incorporo sesion',\n","          'buena tarde',\n","          'persona abandono sesion',\n","          'mediante presente',\n","          'comunicar linea opcion',\n","          'ingrese contrasena', \n","          'ingrese contrasena red seleccione pestana',\n","          'dando clic opcion crear solicitud continue proceso',\n","          'cambio certificado admision carta aceptacion anexar programa academico pensum malla curricular',\n","          'espero pronta respuesta',\n","          'identificado documento identidad numero',\n","          'identificado tarjeta identidad',\n","          'espero pronta respuesta',\n","          'masivo padre',\n","          ]\n","\n","\n","stop_words+=['no','de', 'la', 'el', 'que', 'a', 'en', 'se', 'y', 'por', 'para','donde', 'observe', 'debe', 'fue','un','al', 'le','es' ,'o','con',\n","             'su','lo','legal','consultas','ecp','via','consultas','vía','via','vacio','senora','mb','caso','gestion','este','comité','gracias',\n","             'me','mi','recibe','activos','ya','senor','senora','cc','septiembre','agosto','sobre','como','esta','sr','comunica','buzon',\n","             'cual','debido','indique','remitidos','respectivos','uno','titular','fimrado','anexando','siguiente','recepcion',\n","             'transferencia','si','documentos','siguientes','encargada','area','solo','un','presentar','correo',\n","             'referente','buzon','area encargada','area','encargada','firma','nombre',\n","             'pero','favor','agradezco','agradesco','cordialmente','quiere','par','una','tengo','uno','solo',\n","             'cuarto','lunes','martes','jueves','viernes','sabado','domingo','estara','febrero','conste','funcionario',\n","             'personal','usuario','ano','porque','sra','quien','parte','hace','m','persn','ha','crea', 'diciembre', 'noviembre',\n","             'marzo', 'muchas', 'senores', 'junto', 'cuando', 'usted', 'cct', 'sd', 'aun', 'sin_embargo', 'mis', 'sin', 'estan', 'bien',\n","             'antes', 'les', 'mucho', 'quedo', 'atenta', 'atento', 'hara', 'sera', 'unos', 'yo', 'ahi', 'soy', 'nuestro', 'ese', 'queria',\n","             'algo', 'desde', 'queremos','alli', 'va', 'tiene', 'anos', 'tambien', 'he', 'multiples', 'puede', 'mas', 'dado', 'tib', 'tal',\n","             'hay', 'aqui', 'alla', 'todos', 'tenga', 'cuenta', 'primer', 'hizo', 'hacer', 'do', 'er', 'tomar', 'otra', 'hay',\n","             'aqui', 'alla', 'sino', 'trimestre', 'junio', 'quater', 'enero', 'lleva', 'quaters', 'ninguno', 'cuales', 'son', 'in', 'muy',\n","             'deja', 'dejar', 'ustedes', 'ello', 'ellos', 'ellas','ella', 'gth', 'respetuosamente', 'mes', 'trimestrales', 'anuales', 'segunda',\n","             'veces', 'tus', 'estaba', 'ti', 'enviada', 'realice', 'cenit', 'paso', 'bucaramanga', 'colombia'\n","             ]\n","\n","\n","adjustment ={\"enviar\":\"envio\",\"deseo saber\":\"solicitud\",\"desea saber\":\"solicitud\",\"falta falta\":\"falta\",\"requiere\":\"solicitud\",\n","             \"solicitar\":\"solicitud\",\"solicitada\":\"solicitud\",\"solicita\":\"solicitud\",\" rec \":\" recibo \",\n","              \"informacion reconocimiento derechos grado\":\"solicitud reconocimiento derechos grado\" ,\n","              \"solicitud informacion reconocimiento derechos\":\"solicitud reconocimiento derechos grado\",\n","             \"solicitud solicitud\":\"solicitud\", \"san nicolas\": \"san_nicolas\", \"torrescoordinadora\": \"torres coordinadora\",\n","             \"solicitudmos\":\"solicitud\", 'principio de una esperanza': 'principio_de_una_esperanza', 'plan de formacion':'plan_de_formacion',\n","             \"solicituddo\":\"solicitud\", \"solicito\": \"solicitud\", \"graciasleobaldo\":\"gracias leobaldo\", 'apoyo':'solicitud',\n","             \"sin embargo\":\"sin_embargo\", \"serviciodepersonal\":\"servicio de personal\",\n","             \"oficinavirtualdepersonal\":\"oficina virtual de personal\", \"didculpa\":\"disculpa\", \"enviaste\":\"envio\", \"envie\":\"envio\", \"sii\":\"si\",\n","             \"semestralizafo\":\"semestralizado\", \"queremos saber\":\"solicitud\", \"queria saber\":\"solicitud\", \"solicite\":\"solicitud\",\n","             \"no se genera\":\"no_genera\", \"solicitud reconocimiento\": \"solicitud\", \"solicitud informacion\":\"solicitud\",\n","             \"seleccionar seleccionar\":\"seleccionar\", \"matriculac\":\"matricula\", \"autoservicioingrese\":\"autoservicio ingrese\",\n","             \"redseleccione\":\"red seleccione\", \"educativoseleccione\":\"educativo seleccione\",\n","             \"quiero pedirles\":\"solicitud\", \"tomo\":\"tomar\", \"toma\":\"tomar\", \"hijo\":\"hijo_a\", \"hija\":\"hijo_a\", \"semestres\":\"semestre\",\n","             \"trimestres\":\"trimestre\", \"reconocimiento reconocimiento\":\"reconocimiento\", \"matricula matricula\":\"matricula\",\n","             \"no recibida\":\"no_recibida\", \"materias\":\"materias\"\n","             }\n","\n","adjustment1 ={\"solicitud solicitud\":\"solicitud\", \"quisiera saber\":\"solicitud\"\n","             }\n","\n","\n","nombres_lista = pd.read_csv(nombres, sep=\" \", header = None)\n","stop_words+= nombres_lista[0].to_list()+['ligia' ,'maria' ,'aguilera','murcia' ,'yudi' ,'cuadros' ,'torrado','valeria','Valeria','Nicole','nicole',\n","                                         'francisca','Francisca','isabella','berdugo','sharon', 'lisset', 'lisseth', 'herlyng', 'nicola', 'fernandezc',\n","                                         'verac', 'rolonc', 'traina', 'rinconsd', 'carrasquillareg', 'leobaldo']\n","\n","# pd.DataFrame(guiones).set_index(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmmdwb7GYYVD"},"outputs":[],"source":["#Hay que correrlo dos veces :( ni idea por que\n","def limpieza_guiones(x, guiones_ = guiones):\n","  for i in guiones_: \n","    x =x.replace(i,'')\n","  return x\n","\n","def limpieza_stop(x, stop_words=stop_words):## para palabras individuales\n","  y = x.split(' ')\n","  z = [i for i in  y if i  not in stop_words] \n","  return  ' '.join(z)"]},{"cell_type":"markdown","metadata":{"id":"WGfoFHJcWHBk"},"source":["# '4. Consulta'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKkYkZkrWL95"},"outputs":[],"source":["interes['GRUPOS'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSdfNhM5BNGF"},"outputs":[],"source":["mapa = interes['GRUPOS']=='4. Consulta'\n","grupo = interes[mapa].copy()\n","print(grupo.shape, interes.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nqAj-Z5Ng4-L"},"outputs":[],"source":["#@markdown limpieza guiones\n","grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].map(lambda x : x.lower())\n","grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].map(lambda x : x.lower())\n","\n","\n","\n","grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x : limpieza_guiones(x))\n","grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].astype(str).map(lambda x : limpieza_guiones(x))\n","print(\n","sum(grupo['ObservacionSolucion_+_n2'].map(lambda x : len(x.split(' ')))),\n","sum(grupo['ObservacionCreacion'].map(lambda x : len(x.split(' ')))),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"M643sc87ifpr"},"outputs":[],"source":["#@markdown limpieza sustitucion                                                                                                      \n","grupo['ObservacionSolucion_+_n2']  = grupo['ObservacionSolucion_+_n2'] .map(\n","                            lambda x: adjustment_all(x,  adjustment))\n","grupo['ObservacionCreacion']  = grupo['ObservacionCreacion'] .map(\n","                            lambda x: adjustment_all(x,  adjustment))\n","\n","print(\n","sum(grupo['ObservacionSolucion_+_n2'].map(lambda x : len(x.split(' ')))),\n","sum(grupo['ObservacionCreacion'].map(lambda x : len(x.split(' ')))),\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"azk0dkgXTpC_"},"outputs":[],"source":["#@markdown limpieza stop\n","\n","\n","\n","grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x : limpieza_stop(x))\n","grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].astype(str).map(lambda x : limpieza_stop(x))\n","\n","\n","\n","print(\n","sum(grupo['ObservacionSolucion_+_n2'].map(lambda x : len(x.split(' ')))),\n","sum(grupo['ObservacionCreacion'].map(lambda x : len(x.split(' ')))),\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Aw_f6IrBHcqg"},"outputs":[],"source":["#@markdown limpieza sustitucion (parte 2)                                                                                                      \n","grupo['ObservacionSolucion_+_n2']  = grupo['ObservacionSolucion_+_n2'] .map(\n","                            lambda x: adjustment_all(x,  adjustment1))\n","grupo['ObservacionCreacion']  = grupo['ObservacionCreacion'] .map(\n","                            lambda x: adjustment_all(x,  adjustment1))\n","\n","print(\n","sum(grupo['ObservacionSolucion_+_n2'].map(lambda x : len(x.split(' ')))),\n","sum(grupo['ObservacionCreacion'].map(lambda x : len(x.split(' ')))),\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pR-NWoyfT7nq"},"outputs":[],"source":["#@markdown limpieza\n","\n","\n","\n","# grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].map(lambda x : x.lower())\n","# grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].map(lambda x : x.lower())\n","\n","\n","\n","# grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x : limpieza_guiones(x))\n","# grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].astype(str).map(lambda x : limpieza_guiones(x))\n","\n","# grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x : limpieza_stop(x))\n","# grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].astype(str).map(lambda x : limpieza_stop(x))\n","\n","                                                                                                         \n","# grupo['ObservacionSolucion_+_n2']  = grupo['ObservacionSolucion_+_n2'] .map(\n","#                             lambda x: adjustment_all(x,  adjustment))\n","# grupo['ObservacionCreacion']  = grupo['ObservacionCreacion'] .map(\n","#                             lambda x: adjustment_all(x,  adjustment))\n","\n","\n","\n","# print(\n","# sum(grupo['ObservacionSolucion_+_n2'].map(lambda x : len(x.split(' ')))),\n","# sum(grupo['ObservacionCreacion'].map(lambda x : len(x.split(' ')))),\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5E3WzIZZwU67"},"outputs":[],"source":["grupo['ObservacionSolucion_+_n2'] = grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x : limpieza_guiones(x))\n","grupo['ObservacionCreacion'] = grupo['ObservacionCreacion'].astype(str).map(lambda x : limpieza_guiones(x))\n","\n","\n","print(\n","sum(grupo['ObservacionSolucion_+_n2'].map(lambda x : len(x.split(' ')))),\n","sum(grupo['ObservacionCreacion'].map(lambda x : len(x.split(' ')))),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9W6mIO0pUVQO"},"outputs":[],"source":["grupo['ObservacionSolucion_+_n2']=grupo['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.strip() if type(x)==str else x)\n","grupo['ObservacionCreacion']=grupo['ObservacionCreacion'].astype(str).apply(lambda x: x.strip() if type(x)==str else x)\n","grupo['ObservacionSolucion_+_n2']=grupo['ObservacionSolucion_+_n2'].astype(str).apply(lambda x: x.replace('  ',' ') )\n","grupo['ObservacionCreacion']=grupo['ObservacionCreacion'].astype(str).apply(lambda x: x.replace('  ',' ') )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsZgU3ralNmM"},"outputs":[],"source":["freq_cre = pd.Series(grupo['ObservacionCreacion'].map(lambda x : x.split(' ')).sum()).value_counts().head(10)\n","freq_cre"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AwpuJzmqMOX"},"outputs":[],"source":["freq_sol= pd.Series(grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x : x.split(' ')).sum()).value_counts().head(10)\n","freq_sol"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfOgGYRxJzPn"},"outputs":[],"source":["interes.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ur0mXsNs1F4"},"outputs":[],"source":["# interes['ObservacionCreacion_fgsnivel3'].astype(str).map(lambda x : limpieza_guiones(x))[mapa].values[0]"]},{"cell_type":"markdown","metadata":{"id":"0L32RWXBSjMH"},"source":["### Buscador"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEAieuSdaAbc"},"outputs":[],"source":["#BUSCADOR\n","# metodo para busqueda de plabras ( se puede generar el mapa con el original y revisar en el de grupo.)\n","texto = 'identificado'\n","mapa = grupo['ObservacionSolucion_+_n2'].astype(str).map(lambda x: x.find(texto))!=-1\n","x = pd.DataFrame(grupo[['ObservacionSolucion_+_n2','fgs_IdGestion']][mapa]).set_index('ObservacionSolucion_+_n2')\n","x.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GauB6FMR0hsw"},"outputs":[],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAuMDYU8uG4p"},"outputs":[],"source":["mapa = interes['fgs_IdGestion']==13917441\n","pd.DataFrame(interes[['ObservacionSolucion_+_n2','fgs_IdGestion'\t]][mapa]).set_index('ObservacionSolucion_+_n2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGAE8qQjbeQq"},"outputs":[],"source":["mapa = grupo['fgs_IdGestion']==13956440\n","pd.DataFrame(grupo[['ObservacionSolucion_+_n2','fgs_IdGestion'\t]][mapa]).set_index('ObservacionSolucion_+_n2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foLjJlefz7wG"},"outputs":[],"source":["mapa = grupo['ObservacionCreacion'].astype(str).map(lambda x: x.find(texto))!=-1\n","x = pd.DataFrame(grupo[['ObservacionCreacion','fgs_IdGestion'    ]][mapa]).set_index('ObservacionCreacion')\n","x.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rr-lA-R1MnQ"},"outputs":[],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e6C7dDA6CJv"},"outputs":[],"source":["mapa = interes['fgs_IdGestion']==14007992\n","pd.DataFrame(interes[['ObservacionCreacion','fgs_IdGestion'\t]][mapa]).set_index('ObservacionCreacion')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_N-apLLXhkN"},"outputs":[],"source":["interes.columns"]},{"cell_type":"markdown","metadata":{"id":"w2Up8ctmAMWP"},"source":["### Creacion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GKITe6ATyqp"},"outputs":[],"source":["name = 'Creacion'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5KvH8TZTp4r"},"outputs":[],"source":["df_creacion =grupo['ObservacionCreacion'].copy()\n","df_solucion =grupo['ObservacionSolucion_+_n2'].copy()\n","df_in=df_creacion#+ ' '+ df_solucion\n","df_in.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"404esn9KTp4v"},"outputs":[],"source":["  for i in range(1,6):\n","    ng = i\n","    segmento = str.upper(\"({}_{}-gramas)\".format(name,ng))\n","    # Iniciar el count vectorizer con stop words personalizado en español\n","    count_vectorizer = CountVectorizer(stop_words=[],ngram_range=(ng, ng))\n","    # Ajustar y transformar los términos procesados\n","    count_data = count_vectorizer.fit_transform(df_in)\n","    #Visualizar los términos mas comunes\n","    words,counts = plot_10_most_common_words(count_data, count_vectorizer)\n","    # buen dia , ecompetro com , com .co \n","    print(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WydeBCCrTp4x"},"outputs":[],"source":["df_in.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkM25AQATp4y"},"outputs":[],"source":["df_in=df_in.apply(lambda x: x.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8so_a9TTp4z"},"outputs":[],"source":["df_in.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTy3MjiTTp41"},"outputs":[],"source":["df_in[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bora0nFRTp42"},"outputs":[],"source":["lista_mensajeuser=[]\n","for i in range(len(df_in)):\n","  lista_mensajeuser+=df_in[i]\n","long_string=''.join(lista_mensajeuser)\n","sal = pd.Series(long_string.split(' ')).value_counts()\n","\n","\n","sal2 = sal.reset_index()\n","sal2 = sal2.set_index('index')\n","saldic = sal2[0].to_dict()\n","\n","#Creacion lista stop words\n","wordcloud = WordCloud(background_color=\"white\",min_font_size=5, max_font_size=150, max_words=1000, contour_width=50, contour_color='steelblue', margin=15, \n","                      stopwords=[])\n","#Crear el word cloud\n","#wordcloud.generate(long_string)\n","wordcloud.generate_from_frequencies(saldic)\n","# wordcloud.generate(saldic)\n","#Visualizar el word cloud\n","wordcloud.to_file('personal_patrones.png')\n","wordcloud.to_image()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABwRc-rsTp43"},"outputs":[],"source":["segmento = str.upper(\"{}\".format(name))\n","# Iniciar el count vectorizer con stop words personalizado en español\n","count_vectorizer = CountVectorizer(stop_words=[])\n","# Ajustar y transformar los términos procesados\n","count_data = count_vectorizer.fit_transform(df_in)\n","#Visualizar los términos mas comunes\n","x , y= plot_10_most_common_words(count_data, count_vectorizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHBgDx_lTp44"},"outputs":[],"source":["pd.Series(df_in.map(lambda x : x.split(' ')).sum()).value_counts().head(15).index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53Fghf1gTp45"},"outputs":[],"source":["df_in.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGtLvVXfTp45"},"outputs":[],"source":["dw = list(sent_to_words(df_in))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"da5LI3xWTp46"},"outputs":[],"source":["dw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KK9oqydqTp46"},"outputs":[],"source":["len(dw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJcROm2kTp46"},"outputs":[],"source":["# @title Creación del Corpus\n","%%time\n","\n","nlp = spacy.load('es_core_news_md')\n","\n","#Oraciones a lista de palabras, remocion de puntuacion y caracteres innecesarios\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remueve puctuacion\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in swseg] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","#CREACION DEL CORPUS Y FACTORIZACION DE TERMINOS\n","#Analizando los mensajes del usuario en general\n","data=df_in.tolist()\n","\n","data_words = list(sent_to_words(data))# Se crea una lista de listas\n","\n","#BRIGRAMAS\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Umbral superior de pocas frases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","bigram_mod = gensim.models.phrases.Phraser(gensim.models.Phrases(dw, min_count=5, threshold=100))\n","trigram_mod = gensim.models.phrases.Phraser(gensim.models.Phrases(bigram[dw], threshold=100))\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in lista_stop_words] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","#Definicion de segmento a analizar\n","\n","# data_words_bigrams = make_bigrams(dw)\n","# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","# id2word = corpora.Dictionary(data_lemmatized)\n","# texts = data_lemmatized\n","\n","data_words_bigrams = make_bigrams(dw)\n","# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","# id2word = corpora.Dictionary(data_lemmatized)\n","id2word = corpora.Dictionary(data_words_bigrams)\n","\n","# texts = data_lemmatized\n","texts = data_words_bigrams\n","\n","\n","\n","corpus = [id2word.doc2bow(text) for text in texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QO3O0bSXTp47"},"outputs":[],"source":["# #@title Optimización del Modelo\n","# #Definicion del segmento persistente\n","# segmento = \"Solucion\"\n","\n","# #Funcion de soporte\n","# def compute_coherence_values(corpus, dictionary, k, a, b):\n","    \n","#     lda_model = gensim.models.LdaMulticore(corpus=corpus,\n","#                                            id2word=id2word,\n","#                                            num_topics=10, \n","#                                            random_state=100,\n","#                                            chunksize=100,\n","#                                            passes=10,\n","#                                            alpha=a,\n","#                                            eta=b,\n","#                                            per_word_topics=True)\n","    \n","#     coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","    \n","#     return coherence_model_lda.get_coherence()\n","\n","# grid = {}\n","# grid['Validation_Set'] = {}\n","\n","# #Rango de topicos\n","# min_topics = 2\n","# # max_topics = 11\n","# max_topics = 5\n","# step_size = 1\n","# topics_range = range(min_topics, max_topics, step_size)\n","\n","# #Parametro Alpha\n","# alpha = list(np.arange(0.01, 1, 0.3))\n","# alpha.append('symmetric')\n","# alpha.append('asymmetric')\n","\n","# #Parametro Beta\n","# beta = list(np.arange(0.01, 1, 0.3))\n","# beta.append('symmetric')\n","\n","# #Conjuntos de validacion\n","# num_of_docs = len(corpus)\n","# corpus_sets = [corpus]\n","\n","# corpus_title = ['100% Corpus']\n","\n","# model_results = {'Validation_Set': [],\n","#                  'Topics': [],\n","#                  'Alpha': [],\n","#                  'Beta': [],\n","#                  'Coherence': []\n","#                 }\n","\n","# #Intensivo en tiempo (Promedio de tiempo: 2 Horas)\n","\n","# if 1 == 1:\n","#     pbar = tqdm.tqdm(total=540)\n","    \n","#     for i in range(len(corpus_sets)):\n","#         for k in topics_range:\n","#             for a in alpha:\n","#                 for b in beta:\n","#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n","#                                                   k=k, a=a, b=b)\n","\n","#                     model_results['Validation_Set'].append(corpus_title[i])\n","#                     model_results['Topics'].append(k)\n","#                     model_results['Alpha'].append(a)\n","#                     model_results['Beta'].append(b)\n","#                     model_results['Coherence'].append(cv)\n","                    \n","#                     pbar.update(1)\n","#     pd.DataFrame(model_results).to_csv(segmento+'.csv', index=False)\n","#     pbar.close()\n","\n","# #Persistencia de hiperparámetros\n","# model_results_tabc = pd.DataFrame(model_results)\n","# model_results_tabc.to_csv(segmento + '_model.csv')\n","# print(\"**\", len(model_results_tabc), segmento, \"\\n\")\n","# model_results_tabc.head()\n","\n","# #Mejor Modelo\n","# model_results_tabc = pd.read_csv(segmento + '_model.csv')\n","# best_model = model_results_tabc[model_results_tabc.Coherence == model_results_tabc.Coherence.max()][:1]\n","# #display(best_model.head())\n","\n","# #Hiperparámetros del mejor modelo\n","# Topics = int(best_model.Topics.item())\n","# Alpha = float(best_model.Alpha.item())\n","# Beta = float(best_model.Beta.item())\n","\n","# print(\"HIPERPARÁMETROS DEL MODELO OPTIMIZADO:\")\n","# print('\\nNumero de Tópicos:', Topics)\n","# print('Parámetro Alfa:', Alpha)\n","# print('Parámetro Beta:', Beta)\n","# #esto estaba silenciado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfC06yEMTp48"},"outputs":[],"source":["#@title Modelo LDA Optimizado \n","%%time\n","# se cambio la vesion de pylavis https://stackoverflow.com/questions/66123774/why-pyldavis-graph-does-not-display-topic-keywords-on-the-bar-chart\n","# import pyLDAvis.gensim\n","# %matplotlib inline\n","#Visualización del modelo\n","pyLDAvis.enable_notebook()\n","# pyLDAvis.enable_notebook(local=True)\n","\n","lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=3, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=10,\n","                                           alpha=0.909999999999999,\n","                                           eta=0.61,                                        \n","                                           per_word_topics=True)\n","\n","#Visualizacion de los tópicos generados en el modelo LDA\n","pprint(lda_model.print_topics())\n","doc_lda = lda_model[corpus]\n","\n","#Prerrequisitos minimos para la representacion grafica del modelado de topicos\n","d = id2word\n","c = corpus\n","lda = lda_model\n","\n","#Parametros de visualizacion\n","data = pyLDAvis.gensim.prepare(lda, c, d)\n","\n","#Persistencia de resultados de modelado de tópicos\n","pyLDAvis.save_html(data, 'vis_' +name + '.html')\n","print('\\n\\n')\n","#Visualizacion\n","display(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8629Ii4lTp49"},"outputs":[],"source":["#@title Nubes de Palabras de los Términos clave N Principales en cada Tema\n","# 1. Wordcloud of Top N words in each topic\n","from matplotlib import pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.colors as mcolors\n","\n","cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n","\n","cloud = WordCloud(stopwords=[],\n","                  background_color='white',\n","                  width=2500,\n","                  height=1800,\n","                  max_words=20,\n","                  colormap='tab10',\n","                  color_func=lambda *args, **kwargs: cols[i],\n","                  prefer_horizontal=1.0)\n","\n","topics = lda_model.show_topics(formatted=False)\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15,15), sharex=True, sharey=True)\n","\n","for i, ax in enumerate(axes.flatten()):\n","    fig.add_subplot(ax)\n","    topic_words = dict(topics[i][1])\n","    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n","    plt.gca().imshow(cloud)\n","    plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=15))\n","    plt.gca().axis('off')\n","\n","\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.axis('off')\n","plt.savefig(\"personal_{}_topics_{}_results.png\".format(name,i+1),bbox_inches = 'tight')\n","plt.margins(x=0, y=0)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwRvP6krTp4-"},"outputs":[],"source":["#@title Red Semántica\n","\n","#CREACION DEL CORPUS Y FACTORIZACION DE TERMINOS\n","#Analizando los mensajes del usuario en general\n","data=df_in.tolist()\n","\n","#Oraciones a lista de palabras, remocion de puntuacion y caracteres innecesarios\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remueve puctuacion\n","\n","data_words = list(sent_to_words(data))# Se crea una lista de listas\n","\n","#BRIGRAMAS\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Umbral superior de pocas frases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","# Faster way to get a sentence clubbed as a trigram/bigram\n","bigram_mod = gensim.models.phrases.Phraser(bigram)\n","trigram_mod = gensim.models.phrases.Phraser(trigram)\n","\n","# See trigram example\n","#print(trigram_mod[bigram_mod[data_words[0]]])\n","# Funciones para bigrams, trigrams lemmatization\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","#Aplicacion de las respectivas funciones\n","#Formacion de bigramas\n","data_words_bigrams = make_bigrams(data_words)\n","\n","#Creacion del diccionario\n","id2word = corpora.Dictionary(data_words_bigrams)\n","\n","# Creacion del Corpus\n","texts = data_words_bigrams\n","\n","#Frecuencia termino documento\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","#VEMOS CUANTOS TERMINOS ESTAN EN NUESTRO DICCIONARIO Y CUALES SON LOS MÁS FRECUENTE\n","word_freq = defaultdict(int)\n","for sent in data_words_bigrams:\n","    for i in sent:\n","        word_freq[i] += 10\n","len(word_freq)\n","\n","sorted(word_freq, key=word_freq.get, reverse=True)[50:100]\n","\n","cores = multiprocessing.cpu_count()\n","\n","##Entrenamiento del Modelo\n","w2v_model = Word2Vec(min_count=7, #SE CONSTRUYE EL VOCABULARIO A PARTIR DE LA FRECUENCIA DE APARICION QUE SE DEFINA AQUI\n","                     window=3,#SE AJUSTA EL NUMERO DE PALABRAS ATRAS Y ADELANTE DE LA PALABRA OBJETIVO PARA VECORIZARLA\n","                     size=30, #EL TAMANO DEL VECTOR DE CADA PALABRA\n","                     sample=1e-4, #-5 en principio #ESTE ES EL PARAMETRO QUE HAY QUE AJUSTAR, ES EL MAS SENSIBLE DEL MODELO\n","                     alpha=0.003, \n","                     min_alpha=0.007, \n","                     negative=10,\n","                     workers=cores-1)\n","\n","#Construccion de vocabulario\n","w2v_model.build_vocab(data_words_bigrams, progress_per=10000)\n","\n","#Entrenamiento del modelo\n","a=w2v_model.train(data_words_bigrams, total_examples=w2v_model.corpus_count, epochs=200, report_delay=1)\n","\n","#Terminos similares\n","# w2v_model.wv.most_similar(positive=[\"contrasena\"], topn=10)\n","\n","##FUNCION PARA REDUCIR LA DIMENSION DEL ESPACIO DE PALABRAS Y PODER VISUALIZARLAS\n","def tsnescatterplot(model, word, list_names):\n","    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n","    its list of most similar words, and a list of words.\n","    \"\"\"\n","    arrays = np.empty((0, 30), dtype='f')\n","    word_labels = [word]\n","    color_list  = [\"#25427B\"] #a los colores de la campaña\n","\n","    # adds the vector of the query word\n","    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n","    \n","    # gets list of most similar words\n","    close_words = model.wv.most_similar([word])\n","    \n","    # adds the vector for each of the closest words to the array\n","    for wrd_score in close_words:\n","        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n","        word_labels.append(wrd_score[0])\n","        color_list.append(\"#33BBFF\")\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # adds the vector for each of the words from list_names to the array\n","    for wrd in list_names:\n","        wrd_vector = model.wv.__getitem__([wrd])\n","        word_labels.append(wrd)\n","        color_list.append(\"#EBECED\")\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","        \n","    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n","    reduc = PCA(n_components=10).fit_transform(arrays)\n","    \n","    # Finds t-SNE coordinates for 2 dimensions\n","    np.set_printoptions(suppress=True)\n","    \n","    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n","    freq=[]\n","    for i in word_labels:\n","       freq.append(word_freq[i])\n","\n","    # Sets everything up to plot\n","    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n","                       'y': [y for y in Y[:, 1]],\n","                       'words': word_labels,\n","                       'color': color_list,'freq':freq})\n","    \n","    fig, _ = plt.subplots()\n","    fig.set_size_inches(9, 9)\n","    \n","    # Basic plot\n","    p1 = sns.regplot(data=df,\n","                     x=\"x\",\n","                     y=\"y\",\n","                     fit_reg=False,\n","                     marker=\"o\",\n","                     scatter_kws={'s': df['freq'],\n","                                  'facecolors': df['color']\n","                                 }\n","                    )\n","    \n","    # Adds annotations one by one with a loop\n","    for line in range(0, df.shape[0]):\n","         p1.text(df[\"x\"][line],\n","                 df['y'][line],\n","                 '  ('+ df[\"words\"][line].title()+','+str(df['freq'][line])+')',\n","                 horizontalalignment='left',\n","                 verticalalignment='top', size='small',\n","                 #color=df['color'][line],\n","                 color='black',\n","                 weight='normal'\n","                ).set_size(11)\n","    \n","    plt.axis(emit=True)\n","    plt.xlim(Y[:, 0].min()-10, Y[:, 0].max()+10)\n","    plt.ylim(Y[:, 1].min()-10, Y[:, 1].max()+30)\n","    plt.title('Visualización semántica para {}'.format(word.title()))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"zFgiCkshTp5A"},"outputs":[],"source":["#@title\n","nombre = x[0]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHS6GBCwTp5B"},"outputs":[],"source":["#@title\n","\n","\n","# tsnescatterplot(w2v_model, \"activo\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"activo\"], topn=20)][10:])\n","# plt.savefig(\"cuota_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","#@title\n","nombre = x[1]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ihy7jnQRTp5B"},"outputs":[],"source":["# @title \n","#cuota se replaza por el temino clave que encontramos en las graficas de barras para 1-gramas\n","# tsnescatterplot(w2v_model, \"confirmar\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"confirmar\"], topn=20)][10:])\n","# plt.savefig(\"cuota_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","\n","nombre = x[2]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6w0CGcg6Tp5C"},"outputs":[],"source":["#@title\n","# tsnescatterplot(w2v_model, \"actualizar\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"actualizar\"], topn=20)][10:])\n","# plt.savefig(\"intereses_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","nombre = x[3]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnAcHkJ8Tp5C"},"outputs":[],"source":["#@title\n","# tsnescatterplot(w2v_model, \"codigo\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"codigo\"], topn=20)][10:])\n","# plt.savefig(\"llegando_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","\n","nombre = x[4]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5BT0vrATp5D"},"outputs":[],"source":[" word_vectors = w2v_model.wv\n"," word_vectors.save_word2vec_format('w2vecx_total_{}'.format(name))\n"," #Save model\n","from gensim.scripts.word2vec2tensor import word2vec2tensor \n","word2vec2tensor('w2vecx_total_{}'.format(name), '{}_total_tensor00'.format(name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LJRv56vTp5D"},"outputs":[],"source":["#hacer version  con guines y sin guines que llos dijeron."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLO-EOB4W54o"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y9sm7CGW6n5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oB0kaWTUXFGh"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTN65QGKT2yb"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0--6_0WZjx3"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"7l3Q4DTMZkzJ"},"source":["### Solucion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXOgV3V8ZkzK"},"outputs":[],"source":["name = 'Solucion'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMmE7lniZkzK"},"outputs":[],"source":["df_creacion =grupo['ObservacionCreacion'].copy()\n","df_solucion =grupo['ObservacionSolucion_+_n2'].copy()\n","df_in=df_solucion\n","df_in.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQmGE8PiZkzL"},"outputs":[],"source":["  for i in range(1,21):\n","    ng = i\n","    segmento = str.upper(\"({}_{}-gramas)\".format(name,ng))\n","    # Iniciar el count vectorizer con stop words personalizado en español\n","    count_vectorizer = CountVectorizer(stop_words=[],ngram_range=(ng, ng))\n","    # Ajustar y transformar los términos procesados\n","    count_data = count_vectorizer.fit_transform(df_in)\n","    #Visualizar los términos mas comunes\n","    words,counts = plot_10_most_common_words(count_data, count_vectorizer)\n","    # buen dia , ecompetro com , com .co \n","    print(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28QdWMutZkzM"},"outputs":[],"source":["df_in.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHe5AEXzZkzM"},"outputs":[],"source":["df_in=df_in.apply(lambda x: x.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58qcLqkAZkzM"},"outputs":[],"source":["df_in.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_74gK6a_ZkzN"},"outputs":[],"source":["df_in[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ea6D6jzHZkzN"},"outputs":[],"source":["lista_mensajeuser=[]\n","for i in range(len(df_in)):\n","  lista_mensajeuser+=df_in[i]\n","long_string=''.join(lista_mensajeuser)\n","sal = pd.Series(long_string.split(' ')).value_counts()\n","\n","\n","sal2 = sal.reset_index()\n","sal2 = sal2.set_index('index')\n","saldic = sal2[0].to_dict()\n","\n","#Creacion lista stop words\n","wordcloud = WordCloud(background_color=\"white\",min_font_size=5, max_font_size=150, max_words=1000, contour_width=50, contour_color='steelblue', margin=15, \n","                      stopwords=[])\n","#Crear el word cloud\n","#wordcloud.generate(long_string)\n","wordcloud.generate_from_frequencies(saldic)\n","# wordcloud.generate(saldic)\n","#Visualizar el word cloud\n","wordcloud.to_file('personal_patrones.png')\n","wordcloud.to_image()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSDiHPIAZkzN"},"outputs":[],"source":["segmento = str.upper(\"{}\".format(name))\n","# Iniciar el count vectorizer con stop words personalizado en español\n","count_vectorizer = CountVectorizer(stop_words=[])\n","# Ajustar y transformar los términos procesados\n","count_data = count_vectorizer.fit_transform(df_in)\n","#Visualizar los términos mas comunes\n","x , y= plot_10_most_common_words(count_data, count_vectorizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8viG33qZkzO"},"outputs":[],"source":["pd.Series(df_in.map(lambda x : x.split(' ')).sum()).value_counts().head(15).index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiqjV7T1ZkzO"},"outputs":[],"source":["df_in.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKolrjfnZkzP"},"outputs":[],"source":["dw = list(sent_to_words(df_in))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciKr-yt-ZkzP"},"outputs":[],"source":["dw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7QakmfqwZkzQ"},"outputs":[],"source":["len(dw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuEib___ZkzQ"},"outputs":[],"source":["# @title Creación del Corpus\n","%%time\n","\n","nlp = spacy.load('es_core_news_md')\n","\n","#Oraciones a lista de palabras, remocion de puntuacion y caracteres innecesarios\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remueve puctuacion\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in swseg] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","#CREACION DEL CORPUS Y FACTORIZACION DE TERMINOS\n","#Analizando los mensajes del usuario en general\n","data=df_in.tolist()\n","\n","data_words = list(sent_to_words(data))# Se crea una lista de listas\n","\n","#BRIGRAMAS\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Umbral superior de pocas frases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","bigram_mod = gensim.models.phrases.Phraser(gensim.models.Phrases(dw, min_count=5, threshold=100))\n","trigram_mod = gensim.models.phrases.Phraser(gensim.models.Phrases(bigram[dw], threshold=100))\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in lista_stop_words] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","#Definicion de segmento a analizar\n","\n","# data_words_bigrams = make_bigrams(dw)\n","# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","# id2word = corpora.Dictionary(data_lemmatized)\n","# texts = data_lemmatized\n","\n","data_words_bigrams = make_bigrams(dw)\n","# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","# id2word = corpora.Dictionary(data_lemmatized)\n","id2word = corpora.Dictionary(data_words_bigrams)\n","\n","# texts = data_lemmatized\n","texts = data_words_bigrams\n","\n","\n","\n","corpus = [id2word.doc2bow(text) for text in texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWQTJfO8ZkzQ"},"outputs":[],"source":["# #@title Optimización del Modelo\n","# #Definicion del segmento persistente\n","# segmento = \"Solucion\"\n","\n","# #Funcion de soporte\n","# def compute_coherence_values(corpus, dictionary, k, a, b):\n","    \n","#     lda_model = gensim.models.LdaMulticore(corpus=corpus,\n","#                                            id2word=id2word,\n","#                                            num_topics=10, \n","#                                            random_state=100,\n","#                                            chunksize=100,\n","#                                            passes=10,\n","#                                            alpha=a,\n","#                                            eta=b,\n","#                                            per_word_topics=True)\n","    \n","#     coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","    \n","#     return coherence_model_lda.get_coherence()\n","\n","# grid = {}\n","# grid['Validation_Set'] = {}\n","\n","# #Rango de topicos\n","# min_topics = 2\n","# # max_topics = 11\n","# max_topics = 5\n","# step_size = 1\n","# topics_range = range(min_topics, max_topics, step_size)\n","\n","# #Parametro Alpha\n","# alpha = list(np.arange(0.01, 1, 0.3))\n","# alpha.append('symmetric')\n","# alpha.append('asymmetric')\n","\n","# #Parametro Beta\n","# beta = list(np.arange(0.01, 1, 0.3))\n","# beta.append('symmetric')\n","\n","# #Conjuntos de validacion\n","# num_of_docs = len(corpus)\n","# corpus_sets = [corpus]\n","\n","# corpus_title = ['100% Corpus']\n","\n","# model_results = {'Validation_Set': [],\n","#                  'Topics': [],\n","#                  'Alpha': [],\n","#                  'Beta': [],\n","#                  'Coherence': []\n","#                 }\n","\n","# #Intensivo en tiempo (Promedio de tiempo: 2 Horas)\n","\n","# if 1 == 1:\n","#     pbar = tqdm.tqdm(total=540)\n","    \n","#     for i in range(len(corpus_sets)):\n","#         for k in topics_range:\n","#             for a in alpha:\n","#                 for b in beta:\n","#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n","#                                                   k=k, a=a, b=b)\n","\n","#                     model_results['Validation_Set'].append(corpus_title[i])\n","#                     model_results['Topics'].append(k)\n","#                     model_results['Alpha'].append(a)\n","#                     model_results['Beta'].append(b)\n","#                     model_results['Coherence'].append(cv)\n","                    \n","#                     pbar.update(1)\n","#     pd.DataFrame(model_results).to_csv(segmento+'.csv', index=False)\n","#     pbar.close()\n","\n","# #Persistencia de hiperparámetros\n","# model_results_tabc = pd.DataFrame(model_results)\n","# model_results_tabc.to_csv(segmento + '_model.csv')\n","# print(\"**\", len(model_results_tabc), segmento, \"\\n\")\n","# model_results_tabc.head()\n","\n","# #Mejor Modelo\n","# model_results_tabc = pd.read_csv(segmento + '_model.csv')\n","# best_model = model_results_tabc[model_results_tabc.Coherence == model_results_tabc.Coherence.max()][:1]\n","# #display(best_model.head())\n","\n","# #Hiperparámetros del mejor modelo\n","# Topics = int(best_model.Topics.item())\n","# Alpha = float(best_model.Alpha.item())\n","# Beta = float(best_model.Beta.item())\n","\n","# print(\"HIPERPARÁMETROS DEL MODELO OPTIMIZADO:\")\n","# print('\\nNumero de Tópicos:', Topics)\n","# print('Parámetro Alfa:', Alpha)\n","# print('Parámetro Beta:', Beta)\n","# #esto estaba silenciado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdep9TY-ZkzR"},"outputs":[],"source":["#@title Modelo LDA Optimizado \n","%%time\n","# se cambio la vesion de pylavis https://stackoverflow.com/questions/66123774/why-pyldavis-graph-does-not-display-topic-keywords-on-the-bar-chart\n","# import pyLDAvis.gensim\n","# %matplotlib inline\n","#Visualización del modelo\n","pyLDAvis.enable_notebook()\n","# pyLDAvis.enable_notebook(local=True)\n","\n","lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=3, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=10,\n","                                           alpha=0.909999999999999,\n","                                           eta=0.61,                                        \n","                                           per_word_topics=True)\n","\n","#Visualizacion de los tópicos generados en el modelo LDA\n","pprint(lda_model.print_topics())\n","doc_lda = lda_model[corpus]\n","\n","#Prerrequisitos minimos para la representacion grafica del modelado de topicos\n","d = id2word\n","c = corpus\n","lda = lda_model\n","\n","#Parametros de visualizacion\n","data = pyLDAvis.gensim.prepare(lda, c, d)\n","\n","#Persistencia de resultados de modelado de tópicos\n","pyLDAvis.save_html(data, 'vis_' +name + '.html')\n","print('\\n\\n')\n","#Visualizacion\n","display(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyMKSIqOZkzR"},"outputs":[],"source":["#@title Nubes de Palabras de los Términos clave N Principales en cada Tema\n","# 1. Wordcloud of Top N words in each topic\n","from matplotlib import pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.colors as mcolors\n","\n","cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n","\n","cloud = WordCloud(stopwords=[],\n","                  background_color='white',\n","                  width=2500,\n","                  height=1800,\n","                  max_words=20,\n","                  colormap='tab10',\n","                  color_func=lambda *args, **kwargs: cols[i],\n","                  prefer_horizontal=1.0)\n","\n","topics = lda_model.show_topics(formatted=False)\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15,15), sharex=True, sharey=True)\n","\n","for i, ax in enumerate(axes.flatten()):\n","    fig.add_subplot(ax)\n","    topic_words = dict(topics[i][1])\n","    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n","    plt.gca().imshow(cloud)\n","    plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=15))\n","    plt.gca().axis('off')\n","\n","\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.axis('off')\n","plt.savefig(\"personal_{}_topics_{}_results.png\".format(name,i+1),bbox_inches = 'tight')\n","plt.margins(x=0, y=0)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIBF-tn6ZkzR"},"outputs":[],"source":["#@title Red Semántica\n","\n","#CREACION DEL CORPUS Y FACTORIZACION DE TERMINOS\n","#Analizando los mensajes del usuario en general\n","data=df_in.tolist()\n","\n","#Oraciones a lista de palabras, remocion de puntuacion y caracteres innecesarios\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remueve puctuacion\n","\n","data_words = list(sent_to_words(data))# Se crea una lista de listas\n","\n","#BRIGRAMAS\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Umbral superior de pocas frases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","# Faster way to get a sentence clubbed as a trigram/bigram\n","bigram_mod = gensim.models.phrases.Phraser(bigram)\n","trigram_mod = gensim.models.phrases.Phraser(trigram)\n","\n","# See trigram example\n","#print(trigram_mod[bigram_mod[data_words[0]]])\n","# Funciones para bigrams, trigrams lemmatization\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","#Aplicacion de las respectivas funciones\n","#Formacion de bigramas\n","data_words_bigrams = make_bigrams(data_words)\n","\n","#Creacion del diccionario\n","id2word = corpora.Dictionary(data_words_bigrams)\n","\n","# Creacion del Corpus\n","texts = data_words_bigrams\n","\n","#Frecuencia termino documento\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","#VEMOS CUANTOS TERMINOS ESTAN EN NUESTRO DICCIONARIO Y CUALES SON LOS MÁS FRECUENTE\n","word_freq = defaultdict(int)\n","for sent in data_words_bigrams:\n","    for i in sent:\n","        word_freq[i] += 10\n","len(word_freq)\n","\n","sorted(word_freq, key=word_freq.get, reverse=True)[50:100]\n","\n","cores = multiprocessing.cpu_count()\n","\n","##Entrenamiento del Modelo\n","w2v_model = Word2Vec(min_count=7, #SE CONSTRUYE EL VOCABULARIO A PARTIR DE LA FRECUENCIA DE APARICION QUE SE DEFINA AQUI\n","                     window=3,#SE AJUSTA EL NUMERO DE PALABRAS ATRAS Y ADELANTE DE LA PALABRA OBJETIVO PARA VECORIZARLA\n","                     size=30, #EL TAMANO DEL VECTOR DE CADA PALABRA\n","                     sample=1e-4, #-5 en principio #ESTE ES EL PARAMETRO QUE HAY QUE AJUSTAR, ES EL MAS SENSIBLE DEL MODELO\n","                     alpha=0.003, \n","                     min_alpha=0.007, \n","                     negative=10,\n","                     workers=cores-1)\n","\n","#Construccion de vocabulario\n","w2v_model.build_vocab(data_words_bigrams, progress_per=10000)\n","\n","#Entrenamiento del modelo\n","a=w2v_model.train(data_words_bigrams, total_examples=w2v_model.corpus_count, epochs=200, report_delay=1)\n","\n","#Terminos similares\n","# w2v_model.wv.most_similar(positive=[\"contrasena\"], topn=10)\n","\n","##FUNCION PARA REDUCIR LA DIMENSION DEL ESPACIO DE PALABRAS Y PODER VISUALIZARLAS\n","def tsnescatterplot(model, word, list_names):\n","    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n","    its list of most similar words, and a list of words.\n","    \"\"\"\n","    arrays = np.empty((0, 30), dtype='f')\n","    word_labels = [word]\n","    color_list  = [\"#25427B\"] #a los colores de la campaña\n","\n","    # adds the vector of the query word\n","    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n","    \n","    # gets list of most similar words\n","    close_words = model.wv.most_similar([word])\n","    \n","    # adds the vector for each of the closest words to the array\n","    for wrd_score in close_words:\n","        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n","        word_labels.append(wrd_score[0])\n","        color_list.append(\"#33BBFF\")\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # adds the vector for each of the words from list_names to the array\n","    for wrd in list_names:\n","        wrd_vector = model.wv.__getitem__([wrd])\n","        word_labels.append(wrd)\n","        color_list.append(\"#EBECED\")\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","        \n","    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n","    reduc = PCA(n_components=10).fit_transform(arrays)\n","    \n","    # Finds t-SNE coordinates for 2 dimensions\n","    np.set_printoptions(suppress=True)\n","    \n","    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n","    freq=[]\n","    for i in word_labels:\n","       freq.append(word_freq[i])\n","\n","    # Sets everything up to plot\n","    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n","                       'y': [y for y in Y[:, 1]],\n","                       'words': word_labels,\n","                       'color': color_list,'freq':freq})\n","    \n","    fig, _ = plt.subplots()\n","    fig.set_size_inches(9, 9)\n","    \n","    # Basic plot\n","    p1 = sns.regplot(data=df,\n","                     x=\"x\",\n","                     y=\"y\",\n","                     fit_reg=False,\n","                     marker=\"o\",\n","                     scatter_kws={'s': df['freq'],\n","                                  'facecolors': df['color']\n","                                 }\n","                    )\n","    \n","    # Adds annotations one by one with a loop\n","    for line in range(0, df.shape[0]):\n","         p1.text(df[\"x\"][line],\n","                 df['y'][line],\n","                 '  ('+ df[\"words\"][line].title()+','+str(df['freq'][line])+')',\n","                 horizontalalignment='left',\n","                 verticalalignment='top', size='small',\n","                 #color=df['color'][line],\n","                 color='black',\n","                 weight='normal'\n","                ).set_size(11)\n","    \n","    plt.axis(emit=True)\n","    plt.xlim(Y[:, 0].min()-10, Y[:, 0].max()+10)\n","    plt.ylim(Y[:, 1].min()-10, Y[:, 1].max()+30)\n","    plt.title('Visualización semántica para {}'.format(word.title()))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"GxcjSiquZkzS"},"outputs":[],"source":["#@title\n","nombre = x[0]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BqLyIYUZkzS"},"outputs":[],"source":["#@title\n","\n","\n","# tsnescatterplot(w2v_model, \"activo\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"activo\"], topn=20)][10:])\n","# plt.savefig(\"cuota_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","#@title\n","nombre = x[1]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAJp9zBdZkzS"},"outputs":[],"source":["# @title \n","#cuota se replaza por el temino clave que encontramos en las graficas de barras para 1-gramas\n","# tsnescatterplot(w2v_model, \"confirmar\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"confirmar\"], topn=20)][10:])\n","# plt.savefig(\"cuota_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","\n","nombre = x[2]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TZzbPKoZkzS"},"outputs":[],"source":["#@title\n","# tsnescatterplot(w2v_model, \"actualizar\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"actualizar\"], topn=20)][10:])\n","# plt.savefig(\"intereses_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","nombre = x[3]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvDX3KzZZkzS"},"outputs":[],"source":["#@title\n","# tsnescatterplot(w2v_model, \"codigo\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"codigo\"], topn=20)][10:])\n","# plt.savefig(\"llegando_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","\n","nombre = x[4]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbGbb0p8ZkzT"},"outputs":[],"source":[" word_vectors = w2v_model.wv\n"," word_vectors.save_word2vec_format('w2vecx_total_{}'.format(name))\n"," #Save model\n","from gensim.scripts.word2vec2tensor import word2vec2tensor \n","word2vec2tensor('w2vecx_total_{}'.format(name), '{}_total_tensor00'.format(name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IR4luBqiZkzT"},"outputs":[],"source":["#hacer version  con guines y sin guines que llos dijeron."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OROeNFZZkzT"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HFrVLLFZkzT"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbzXnRYWZkzT"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmIb9sWxZkzT"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"TrkO79BYZm6M"},"source":["### Total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66wgozXiZm6M"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMtqAdrkZm6M"},"outputs":[],"source":["name = 'Total'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01iaaNDqZm6M"},"outputs":[],"source":["df_creacion =grupo['ObservacionCreacion'].copy()\n","df_solucion =grupo['ObservacionSolucion_+_n2'].copy()\n","df_in=df_creacion+ ' '+ df_solucion\n","df_in.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LvaSlp5Zm6N"},"outputs":[],"source":["  for i in range(1,6):\n","    ng = i\n","    segmento = str.upper(\"({}_{}-gramas)\".format(name,ng))\n","    # Iniciar el count vectorizer con stop words personalizado en español\n","    count_vectorizer = CountVectorizer(stop_words=[],ngram_range=(ng, ng))\n","    # Ajustar y transformar los términos procesados\n","    count_data = count_vectorizer.fit_transform(df_in)\n","    #Visualizar los términos mas comunes\n","    words,counts = plot_10_most_common_words(count_data, count_vectorizer)\n","    # buen dia , ecompetro com , com .co \n","    print(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4xEcTD6Zm6N"},"outputs":[],"source":["df_in.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73rw0xWWZm6N"},"outputs":[],"source":["df_in=df_in.apply(lambda x: x.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dx9X8xbHZm6N"},"outputs":[],"source":["df_in.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Oge3TXLZm6N"},"outputs":[],"source":["df_in[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMKrOYkmZm6O"},"outputs":[],"source":["lista_mensajeuser=[]\n","for i in range(len(df_in)):\n","  lista_mensajeuser+=df_in[i]\n","long_string=''.join(lista_mensajeuser)\n","sal = pd.Series(long_string.split(' ')).value_counts()\n","\n","\n","sal2 = sal.reset_index()\n","sal2 = sal2.set_index('index')\n","saldic = sal2[0].to_dict()\n","\n","#Creacion lista stop words\n","wordcloud = WordCloud(background_color=\"white\",min_font_size=5, max_font_size=150, max_words=1000, contour_width=50, contour_color='steelblue', margin=15, \n","                      stopwords=[])\n","#Crear el word cloud\n","#wordcloud.generate(long_string)\n","wordcloud.generate_from_frequencies(saldic)\n","# wordcloud.generate(saldic)\n","#Visualizar el word cloud\n","wordcloud.to_file('personal_patrones.png')\n","wordcloud.to_image()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_5u2M_FZm6O"},"outputs":[],"source":["segmento = str.upper(\"{}\".format(name))\n","# Iniciar el count vectorizer con stop words personalizado en español\n","count_vectorizer = CountVectorizer(stop_words=[])\n","# Ajustar y transformar los términos procesados\n","count_data = count_vectorizer.fit_transform(df_in)\n","#Visualizar los términos mas comunes\n","x , y= plot_10_most_common_words(count_data, count_vectorizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta1x-K4kZm6O"},"outputs":[],"source":["pd.Series(df_in.map(lambda x : x.split(' ')).sum()).value_counts().head(15).index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dv2fgjVRZm6R"},"outputs":[],"source":["df_in.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDrWKzyFZm6R"},"outputs":[],"source":["dw = list(sent_to_words(df_in))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-wN71oBZm6R"},"outputs":[],"source":["dw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rh4eL46oZm6S"},"outputs":[],"source":["len(dw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwnI13pBZm6S"},"outputs":[],"source":["# @title Creación del Corpus\n","%%time\n","\n","nlp = spacy.load('es_core_news_md')\n","\n","#Oraciones a lista de palabras, remocion de puntuacion y caracteres innecesarios\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remueve puctuacion\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in swseg] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","#CREACION DEL CORPUS Y FACTORIZACION DE TERMINOS\n","#Analizando los mensajes del usuario en general\n","data=df_in.tolist()\n","\n","data_words = list(sent_to_words(data))# Se crea una lista de listas\n","\n","#BRIGRAMAS\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Umbral superior de pocas frases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","bigram_mod = gensim.models.phrases.Phraser(gensim.models.Phrases(dw, min_count=5, threshold=100))\n","trigram_mod = gensim.models.phrases.Phraser(gensim.models.Phrases(bigram[dw], threshold=100))\n","\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in lista_stop_words] for doc in texts]\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","#Definicion de segmento a analizar\n","\n","# data_words_bigrams = make_bigrams(dw)\n","# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","# id2word = corpora.Dictionary(data_lemmatized)\n","# texts = data_lemmatized\n","\n","data_words_bigrams = make_bigrams(dw)\n","# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","# id2word = corpora.Dictionary(data_lemmatized)\n","id2word = corpora.Dictionary(data_words_bigrams)\n","\n","# texts = data_lemmatized\n","texts = data_words_bigrams\n","\n","\n","\n","corpus = [id2word.doc2bow(text) for text in texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jBAsWPkZm6T"},"outputs":[],"source":["# #@title Optimización del Modelo\n","# #Definicion del segmento persistente\n","# segmento = \"Solucion\"\n","\n","# #Funcion de soporte\n","# def compute_coherence_values(corpus, dictionary, k, a, b):\n","    \n","#     lda_model = gensim.models.LdaMulticore(corpus=corpus,\n","#                                            id2word=id2word,\n","#                                            num_topics=10, \n","#                                            random_state=100,\n","#                                            chunksize=100,\n","#                                            passes=10,\n","#                                            alpha=a,\n","#                                            eta=b,\n","#                                            per_word_topics=True)\n","    \n","#     coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","    \n","#     return coherence_model_lda.get_coherence()\n","\n","# grid = {}\n","# grid['Validation_Set'] = {}\n","\n","# #Rango de topicos\n","# min_topics = 2\n","# # max_topics = 11\n","# max_topics = 5\n","# step_size = 1\n","# topics_range = range(min_topics, max_topics, step_size)\n","\n","# #Parametro Alpha\n","# alpha = list(np.arange(0.01, 1, 0.3))\n","# alpha.append('symmetric')\n","# alpha.append('asymmetric')\n","\n","# #Parametro Beta\n","# beta = list(np.arange(0.01, 1, 0.3))\n","# beta.append('symmetric')\n","\n","# #Conjuntos de validacion\n","# num_of_docs = len(corpus)\n","# corpus_sets = [corpus]\n","\n","# corpus_title = ['100% Corpus']\n","\n","# model_results = {'Validation_Set': [],\n","#                  'Topics': [],\n","#                  'Alpha': [],\n","#                  'Beta': [],\n","#                  'Coherence': []\n","#                 }\n","\n","# #Intensivo en tiempo (Promedio de tiempo: 2 Horas)\n","\n","# if 1 == 1:\n","#     pbar = tqdm.tqdm(total=540)\n","    \n","#     for i in range(len(corpus_sets)):\n","#         for k in topics_range:\n","#             for a in alpha:\n","#                 for b in beta:\n","#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n","#                                                   k=k, a=a, b=b)\n","\n","#                     model_results['Validation_Set'].append(corpus_title[i])\n","#                     model_results['Topics'].append(k)\n","#                     model_results['Alpha'].append(a)\n","#                     model_results['Beta'].append(b)\n","#                     model_results['Coherence'].append(cv)\n","                    \n","#                     pbar.update(1)\n","#     pd.DataFrame(model_results).to_csv(segmento+'.csv', index=False)\n","#     pbar.close()\n","\n","# #Persistencia de hiperparámetros\n","# model_results_tabc = pd.DataFrame(model_results)\n","# model_results_tabc.to_csv(segmento + '_model.csv')\n","# print(\"**\", len(model_results_tabc), segmento, \"\\n\")\n","# model_results_tabc.head()\n","\n","# #Mejor Modelo\n","# model_results_tabc = pd.read_csv(segmento + '_model.csv')\n","# best_model = model_results_tabc[model_results_tabc.Coherence == model_results_tabc.Coherence.max()][:1]\n","# #display(best_model.head())\n","\n","# #Hiperparámetros del mejor modelo\n","# Topics = int(best_model.Topics.item())\n","# Alpha = float(best_model.Alpha.item())\n","# Beta = float(best_model.Beta.item())\n","\n","# print(\"HIPERPARÁMETROS DEL MODELO OPTIMIZADO:\")\n","# print('\\nNumero de Tópicos:', Topics)\n","# print('Parámetro Alfa:', Alpha)\n","# print('Parámetro Beta:', Beta)\n","# #esto estaba silenciado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SRiAYskZm6W"},"outputs":[],"source":["#@title Modelo LDA Optimizado \n","%%time\n","# se cambio la vesion de pylavis https://stackoverflow.com/questions/66123774/why-pyldavis-graph-does-not-display-topic-keywords-on-the-bar-chart\n","# import pyLDAvis.gensim\n","# %matplotlib inline\n","#Visualización del modelo\n","pyLDAvis.enable_notebook()\n","# pyLDAvis.enable_notebook(local=True)\n","\n","lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=3, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=100,\n","                                           passes=10,\n","                                           alpha=0.909999999999999,\n","                                           eta=0.61,                                        \n","                                           per_word_topics=True)\n","\n","#Visualizacion de los tópicos generados en el modelo LDA\n","pprint(lda_model.print_topics())\n","doc_lda = lda_model[corpus]\n","\n","#Prerrequisitos minimos para la representacion grafica del modelado de topicos\n","d = id2word\n","c = corpus\n","lda = lda_model\n","\n","#Parametros de visualizacion\n","data = pyLDAvis.gensim.prepare(lda, c, d)\n","\n","#Persistencia de resultados de modelado de tópicos\n","pyLDAvis.save_html(data, 'vis_' +name + '.html')\n","print('\\n\\n')\n","#Visualizacion\n","display(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOWo9T9uZm6W"},"outputs":[],"source":["#@title Nubes de Palabras de los Términos clave N Principales en cada Tema\n","# 1. Wordcloud of Top N words in each topic\n","from matplotlib import pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.colors as mcolors\n","\n","cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n","\n","cloud = WordCloud(stopwords=[],\n","                  background_color='white',\n","                  width=2500,\n","                  height=1800,\n","                  max_words=20,\n","                  colormap='tab10',\n","                  color_func=lambda *args, **kwargs: cols[i],\n","                  prefer_horizontal=1.0)\n","\n","topics = lda_model.show_topics(formatted=False)\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15,15), sharex=True, sharey=True)\n","\n","for i, ax in enumerate(axes.flatten()):\n","    fig.add_subplot(ax)\n","    topic_words = dict(topics[i][1])\n","    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n","    plt.gca().imshow(cloud)\n","    plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=15))\n","    plt.gca().axis('off')\n","\n","\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.axis('off')\n","plt.savefig(\"personal_{}_topics_{}_results.png\".format(name,i+1),bbox_inches = 'tight')\n","plt.margins(x=0, y=0)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1J5nvY-Zm6W"},"outputs":[],"source":["#@title Red Semántica\n","\n","#CREACION DEL CORPUS Y FACTORIZACION DE TERMINOS\n","#Analizando los mensajes del usuario en general\n","data=df_in.tolist()\n","\n","#Oraciones a lista de palabras, remocion de puntuacion y caracteres innecesarios\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remueve puctuacion\n","\n","data_words = list(sent_to_words(data))# Se crea una lista de listas\n","\n","#BRIGRAMAS\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Umbral superior de pocas frases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","# Faster way to get a sentence clubbed as a trigram/bigram\n","bigram_mod = gensim.models.phrases.Phraser(bigram)\n","trigram_mod = gensim.models.phrases.Phraser(trigram)\n","\n","# See trigram example\n","#print(trigram_mod[bigram_mod[data_words[0]]])\n","# Funciones para bigrams, trigrams lemmatization\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","\n","#Aplicacion de las respectivas funciones\n","#Formacion de bigramas\n","data_words_bigrams = make_bigrams(data_words)\n","\n","#Creacion del diccionario\n","id2word = corpora.Dictionary(data_words_bigrams)\n","\n","# Creacion del Corpus\n","texts = data_words_bigrams\n","\n","#Frecuencia termino documento\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","#VEMOS CUANTOS TERMINOS ESTAN EN NUESTRO DICCIONARIO Y CUALES SON LOS MÁS FRECUENTE\n","word_freq = defaultdict(int)\n","for sent in data_words_bigrams:\n","    for i in sent:\n","        word_freq[i] += 10\n","len(word_freq)\n","\n","sorted(word_freq, key=word_freq.get, reverse=True)[50:100]\n","\n","cores = multiprocessing.cpu_count()\n","\n","##Entrenamiento del Modelo\n","w2v_model = Word2Vec(min_count=7, #SE CONSTRUYE EL VOCABULARIO A PARTIR DE LA FRECUENCIA DE APARICION QUE SE DEFINA AQUI\n","                     window=3,#SE AJUSTA EL NUMERO DE PALABRAS ATRAS Y ADELANTE DE LA PALABRA OBJETIVO PARA VECORIZARLA\n","                     size=30, #EL TAMANO DEL VECTOR DE CADA PALABRA\n","                     sample=1e-4, #-5 en principio #ESTE ES EL PARAMETRO QUE HAY QUE AJUSTAR, ES EL MAS SENSIBLE DEL MODELO\n","                     alpha=0.003, \n","                     min_alpha=0.007, \n","                     negative=10,\n","                     workers=cores-1)\n","\n","#Construccion de vocabulario\n","w2v_model.build_vocab(data_words_bigrams, progress_per=10000)\n","\n","#Entrenamiento del modelo\n","a=w2v_model.train(data_words_bigrams, total_examples=w2v_model.corpus_count, epochs=200, report_delay=1)\n","\n","#Terminos similares\n","# w2v_model.wv.most_similar(positive=[\"contrasena\"], topn=10)\n","\n","##FUNCION PARA REDUCIR LA DIMENSION DEL ESPACIO DE PALABRAS Y PODER VISUALIZARLAS\n","def tsnescatterplot(model, word, list_names):\n","    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n","    its list of most similar words, and a list of words.\n","    \"\"\"\n","    arrays = np.empty((0, 30), dtype='f')\n","    word_labels = [word]\n","    color_list  = [\"#25427B\"] #a los colores de la campaña\n","\n","    # adds the vector of the query word\n","    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n","    \n","    # gets list of most similar words\n","    close_words = model.wv.most_similar([word])\n","    \n","    # adds the vector for each of the closest words to the array\n","    for wrd_score in close_words:\n","        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n","        word_labels.append(wrd_score[0])\n","        color_list.append(\"#33BBFF\")\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","    \n","    # adds the vector for each of the words from list_names to the array\n","    for wrd in list_names:\n","        wrd_vector = model.wv.__getitem__([wrd])\n","        word_labels.append(wrd)\n","        color_list.append(\"#EBECED\")\n","        arrays = np.append(arrays, wrd_vector, axis=0)\n","        \n","    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n","    reduc = PCA(n_components=10).fit_transform(arrays)\n","    \n","    # Finds t-SNE coordinates for 2 dimensions\n","    np.set_printoptions(suppress=True)\n","    \n","    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n","    freq=[]\n","    for i in word_labels:\n","       freq.append(word_freq[i])\n","\n","    # Sets everything up to plot\n","    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n","                       'y': [y for y in Y[:, 1]],\n","                       'words': word_labels,\n","                       'color': color_list,'freq':freq})\n","    \n","    fig, _ = plt.subplots()\n","    fig.set_size_inches(9, 9)\n","    \n","    # Basic plot\n","    p1 = sns.regplot(data=df,\n","                     x=\"x\",\n","                     y=\"y\",\n","                     fit_reg=False,\n","                     marker=\"o\",\n","                     scatter_kws={'s': df['freq'],\n","                                  'facecolors': df['color']\n","                                 }\n","                    )\n","    \n","    # Adds annotations one by one with a loop\n","    for line in range(0, df.shape[0]):\n","         p1.text(df[\"x\"][line],\n","                 df['y'][line],\n","                 '  ('+ df[\"words\"][line].title()+','+str(df['freq'][line])+')',\n","                 horizontalalignment='left',\n","                 verticalalignment='top', size='small',\n","                 #color=df['color'][line],\n","                 color='black',\n","                 weight='normal'\n","                ).set_size(11)\n","    \n","    plt.axis(emit=True)\n","    plt.xlim(Y[:, 0].min()-10, Y[:, 0].max()+10)\n","    plt.ylim(Y[:, 1].min()-10, Y[:, 1].max()+30)\n","    plt.title('Visualización semántica para {}'.format(word.title()))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"leHY70CUZm6X"},"outputs":[],"source":["#@title\n","nombre = x[0]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tr83AHOlZm6X"},"outputs":[],"source":["#@title\n","\n","\n","# tsnescatterplot(w2v_model, \"activo\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"activo\"], topn=20)][10:])\n","# plt.savefig(\"cuota_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","#@title\n","nombre = x[1]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYcJlAtsZm6X"},"outputs":[],"source":["# @title \n","#cuota se replaza por el temino clave que encontramos en las graficas de barras para 1-gramas\n","# tsnescatterplot(w2v_model, \"confirmar\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"confirmar\"], topn=20)][10:])\n","# plt.savefig(\"cuota_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","\n","nombre = x[2]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nU5eIeaZZm6X"},"outputs":[],"source":["#@title\n","# tsnescatterplot(w2v_model, \"actualizar\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"actualizar\"], topn=20)][10:])\n","# plt.savefig(\"intereses_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","nombre = x[3]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoczKVSOZm6X"},"outputs":[],"source":["#@title\n","# tsnescatterplot(w2v_model, \"codigo\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"codigo\"], topn=20)][10:])\n","# plt.savefig(\"llegando_similaridad_calidad_personal_certificado.png\",bbox_inches = 'tight')\n","\n","nombre = x[4]\n","tsnescatterplot(w2v_model, nombre, [t[0] for t in w2v_model.wv.most_similar(positive=[nombre], topn=20)][10:])\n","plt.savefig(nombre+\".png\",bbox_inches = 'tight')\n","##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BoTfR0hZm6Y"},"outputs":[],"source":[" word_vectors = w2v_model.wv\n"," word_vectors.save_word2vec_format('w2vecx_total_{}'.format(name))\n"," #Save model\n","from gensim.scripts.word2vec2tensor import word2vec2tensor \n","word2vec2tensor('w2vecx_total_{}'.format(name), '{}_total_tensor00'.format(name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7VT45bbZm6Y"},"outputs":[],"source":["#hacer version  con guines y sin guines que llos dijeron."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LU96xSQZm6Y"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_CBzVWMZm6Y"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTpwk2BgZm6Y"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aV9JTIR8Zm6Y"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":["64b4yJA8DeDn","yzY6YoaOs2IE","Lb5LH48D0qL_"],"name":"Consulta.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}